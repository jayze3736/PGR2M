nohup python eval_t2m.py \
--exp-name TEST_exp_2025_10_26_t2m_transformer_ver_update_loss_v2 \
--batch-size 32 \
--num-layers 9 \
--embed-dim-gpt 1024 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--resume-pth ./pretrained/t2m/Dec/net_best_fid.pth \
--vq-name VQVAE_2025_10_15 \
--out-dir eval_output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--vq-act relu \
--codes-folder-name codes \
--output-emb-width 392 \
--resume-trans ./output/exp_2025_10_24_t2m_transformer_vanilla_update_loss_v2/2025-10-24_04-35-01/net_best_fid.pth \
--use-keywords > log_eval_exp_2025_10_26_t2m_transformer_ver_update_loss_v2.log &


nohup python train_t2m_update_loss.py \
--exp-name exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m_v2/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2.log &

nohup python train_t2m_update_loss.py \
--exp-name exp_2025_10_25_t2m_transformer_vanilla_update_loss_focal_bce_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m_v2/losses_v2.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_exp_2025_10_25_t2m_transformer_vanilla_update_loss_focal_bce_loss.log &


nohup python train_t2m_update_loss.py \
--exp-name exp_2025_10_25_t2m_transformer_vanilla_update_loss_focal_bce_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m_v2/losses_v2.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_exp_2025_10_25_t2m_transformer_vanilla_update_loss_focal_bce_loss.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_no_pos_emb_on_cond_ver_pad_mask_updated_v14_scheduled_learning \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--share-weight \
--schedule-masking-prob \
--schedule-pkeep \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_no_pos_emb_on_cond_ver_pad_mask_updated_v14_scheduled_learning.log &



nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v12_no_pos_emb_on_cond \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--clip-grad \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v12_no_pos_emb_on_cond.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v12 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--clip-grad \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v12.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v11 \
--batch-size 128 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--clip-grad \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_24 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_24 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v11.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v10 \
--batch-size 128 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--schedule-masking-prob \
--schedule-pkeep \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v10.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v9 \
--batch-size 128 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--schedule-masking-prob \
--start-warm-up \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v9.log &

nohup python train_t2m_update_loss.py \
--exp-name exp_2025_10_24_t2m_transformer_vanilla_update_loss_v2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m_v2/losses_v2.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_exp_2025_10_24_t2m_transformer_vanilla_update_loss_v2.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v7 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--schedule-masking-prob \
--start-warm-up \
--schedule-pkeep \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v7.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v6 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v6.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v3 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v3.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v2 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.7 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling_v2.log &


nohup python train_t2m.py \
--exp-name TEST_exp_2025_10_23_t2m_transformer_vanilla_v4 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > TEST_log_2025_10_23_t2m_transformer_vanilla_v4.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_23_rt2m_transformer_vanilla_ver_masked_modeling.log &

# 
nohup python train_t2m.py \
--exp-name exp_2025_10_19_t2m_transformer_ver_bce_loss_ver_codebook_init_embedding_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--token-emb-layer codebook-init \
--froze-codebook \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--resume-trans ./output/exp_2025_10_19_t2m_transformer_ver_bce_loss_ver_codebook_init_embedding/2025-10-19_08-57-38/net_last.pth \
--use-keywords > log_2025_10_19_t2m_transformer_ver_bce_loss_ver_codebook_init_embedding_resume.log &

nohup python eval_t2m.py \
--exp-name TEST_exp_2025_10_11_t2m_transformer_ver_focal_bce_loss \
--batch-size 32 \
--num-layers 9 \
--embed-dim-gpt 1024 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--resume-pth ./pretrained/t2m/Dec/net_best_fid.pth \
--vq-name VQVAE_2025_10_15 \
--out-dir eval_output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--vq-act relu \
--codes-folder-name codes \
--output-emb-width 392 \
--resume-trans ./output/exp_2025_10_11_t2m_transformer_ver_focal_bce_loss/2025-10-11_16-50-50/net_best_fid.pth \
--use-keywords > log_eval_2025_10_11_t2m_transformer_ver_focal_bce_loss.log &



nohup python eval_t2m.py \
--exp-name TEST_exp_2025_10_15_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking_ver_random_schedule \
--batch-size 32 \
--num-layers 9 \
--embed-dim-gpt 1024 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--resume-pth ./pretrained/t2m/Dec/net_best_fid.pth \
--vq-name VQVAE_2025_10_15 \
--out-dir eval_output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--vq-act relu \
--codes-folder-name codes \
--output-emb-width 392 \
--resume-trans ./output/exp_2025_10_15_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking_ver_random_schedule/2025-10-15_03-25-00/net_best_fid.pth \
--use-keywords > log_eval_2025_10_15_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking_ver_random_schedule.log &

# 
nohup python train_rt2m.py \
--exp-name exp_2025_10_22_rt2m_transformer_vanilla_ver_masked_modeling \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_22_rt2m_transformer_vanilla_ver_masked_modeling.log &

--eval-masking \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_21 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_21 \

# 2025 10 20
nohup python train_t2m.py \
--exp-name exp_2025_10_20_t2m_transformer_ver_bce_loss_ver_text_graph_based_reasoning_ver_block_attend_cond2cond \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--clip-dim 768 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--block-attend-cond2cond \
--text-encoding-method graph_reasoning \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_20_t2m_transformer_ver_bce_loss_ver_text_graph_based_reasoning_ver_block_attend_cond2cond.log &

nohup python train_t2m.py \
--exp-name exp_2025_10_20_t2m_transformer_ver_bce_loss_ver_text_graph_based_reasoning \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--clip-dim 768 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--text-encoding-method graph_reasoning \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_20_t2m_transformer_ver_bce_loss_ver_text_graph_based_reasoning.log &

--block-attend-cond2cond \

# 2025 10 19
nohup python train_t2m.py \
--exp-name exp_2025_10_19_t2m_transformer_ver_bce_loss_ver_num_head_8 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_19_t2m_transformer_ver_bce_loss_ver_num_head_8.log &


nohup python train_t2m.py \
--exp-name exp_2025_10_19_t2m_transformer_ver_bce_loss_ver_codebook_init_embedding \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--token-emb-layer codebook-init \
--froze-codebook \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_19_t2m_transformer_ver_bce_loss_ver_codebook_init_embedding.log &

# 2025 10 15(예정)

nohup python train_t2m.py \
--exp-name exp_2025_10_15_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking_ver_random_schedule \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v9.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--mask-method confidence_based_symmetric_distance \
--max-gamma 1.5 \
--min-gamma 0.1 \
--gamma-schedule random \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_15_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking_ver_random_schedule.log &


# 2025 10 12(예정)

## 점수화
nohup python train_t2m.py \
--exp-name exp_2025_10_13_t2m_transformer_ver_focal_cace_kl_loss_ver_use_difficulty_reward \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v11.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_13_t2m_transformer_ver_focal_cace_kl_loss_ver_use_difficulty_reward.log &

nohup python train_t2m.py \
--exp-name exp_2025_10_12_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v9.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--mask-method confidence_based_symmetric_distance \
--max-gamma 1.5 \
--min-gamma 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_12_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking.log &

nohup python train_t2m.py \
--exp-name exp_2025_10_12_t2m_transformer_ver_focal_cace_kl_loss_conf_based_symmetric_masking \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v10.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--mask-method confidence_based_symmetric_distance \
--max-gamma 1.5 \
--min-gamma 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_12_t2m_transformer_ver_focal_cace_kl_loss_conf_based_symmetric_masking.log &


# 2025 10 11
nohup python train_t2m.py \
--exp-name exp_2025_10_11_t2m_transformer_ver_focal_bce_loss_init_prior \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v9.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--init-prior \
--use-keywords > log_2025_10_11_t2m_transformer_ver_focal_bce_loss_init_prior.log &

nohup python train_t2m.py \
--exp-name exp_2025_10_11_t2m_transformer_ver_focal_cace_kl_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v10.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_11_t2m_transformer_ver_focal_cace_kl_loss.log &


nohup python train_t2m.py \
--exp-name exp_2025_10_11_t2m_transformer_ver_focal_bce_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v9.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_11_t2m_transformer_ver_focal_bce_loss.log &

--dont-corrupt-pad-end \


# 2025 10 09
nohup python train_t2m.py \
--exp-name exp_2025_10_09_ver_symmetric_distance_based_masking \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--mask-method symmetric_distance \
--mask-gamma 1.1 \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_09_ver_symmetric_distance_based_masking.log &



nohup python train_t2m.py \
--exp-name exp_2025_10_02_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--min-sampling-prob 0.1 \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-trans /data/CoMo/output/exp_2025_10_02_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-10-02_05-51-49/net_last.pth \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_02_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_resume.log &



nohup python train_t2m.py \
--exp-name exp_2025_10_09_t2m_transformer_ver_cace_kl_loss_ver_supervise_only_angle_and_distance \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--debug-target-type-list angle distance \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_09_t2m_transformer_ver_cace_kl_loss_ver_supervise_only_angle_and_distance.log &


# 2025 10 08
nohup python train_t2m_with_cross_attn_ver.py \
--exp-name exp_2025_10_08_t2m_transformer_rptc_decoder_2025_09_27_Dec_ver_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_512_nb_q_layer_6_drop_p_ver_2_ver_sincos_emb_ver_cross_attn \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 51 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--cond-mask-prob 0.0 \
--dont-corrupt-pad-end \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_08_t2m_transformer_rptc_decoder_2025_09_27_Dec_ver_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_512_nb_q_layer_6_drop_p_ver_2_ver_sincos_emb_ver_cross_attn.log &


nohup python train_t2m.py \
--exp-name exp_2025_10_03_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-rope-pos-emb \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_03_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb.log &


nohup python train_t2m.py \
--exp-name exp_2025_10_08_t2m_transformer_vanilla \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_2025_10_08_t2m_transformer_vanilla.log &

nohup python train_t2m_with_cross_attn_ver.py \
--exp-name exp_2025_10_08_t2m_transformer_rptc_decoder_2025_09_27_Dec_ver_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_512_nb_q_layer_6_drop_p_ver_2_ver_sincos_emb_ver_cross_attn \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 51 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--cond-mask-prob 0.0 \
--dont-corrupt-pad-end \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_08_t2m_transformer_rptc_decoder_2025_09_27_Dec_ver_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_512_nb_q_layer_6_drop_p_ver_2_ver_sincos_emb_ver_cross_attn.log &

# 2025 10 07


nohup python train_t2m.py \
--exp-name exp_2025_10_07_t2m_transformer_rptc_decoder_2025_09_27_Dec_ver_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_512_nb_q_layer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb_ver_mask_only_motion_tokens_ver_mask_keywords \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-rope-pos-emb \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--pos-emb-rope-offset 12 \
--mask-only-motion-tokens \
--cond-mask-prob 0.1 \
--dont-corrupt-pad-end \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_07_t2m_transformer_rptc_decoder_2025_09_27_Dec_ver_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_512_nb_q_layer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb_ver_mask_only_motion_tokens_ver_mask_keywords.log &

# 2025 10 06
nohup python train_t2m.py \
--exp-name exp_2025_10_06_t2m_transformer_with_rptc_decoder_rptc_ver_res_drop_with_soft_quantizer_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb_with_soft_label_tau_0p5_error_tol_1_kl_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-rope-pos-emb \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--dont-corrupt-pad-end \
--soft-label-folder-name soft_labeling_based_symmetric_descending/tau_0.5_error_tol_1 \
--pos-emb-rope-offset 12 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_06_t2m_transformer_with_rptc_decoder_rptc_ver_res_drop_with_soft_quantizer_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb_with_soft_label_tau_0p5_error_tol_1_kl_loss.log &



# 2025 10 05
nohup python train_t2m.py \
--exp-name exp_2025_10_05_t2m_transformer_with_rptc_decoder_rptc_ver_res_drop_with_soft_quantizer_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb_with_soft_label_tau_0p5 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-rope-pos-emb \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--dont-corrupt-pad-end \
--soft-label-folder-name soft_labeling_based_symmetric_descending/tau_0.5 \
--pos-emb-rope-offset 12 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_05_t2m_transformer_with_rptc_decoder_rptc_ver_res_drop_with_soft_quantizer_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb_with_soft_label_tau_0p5.log &


# 2025 10 04

nohup python train_t2m.py \
--exp-name exp_2025_10_04_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-rope-pos-emb \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--pos-emb-rope-offset 12 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_04_t2m_transformer_vanilla_with_rptc_decoder_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb_no_cond_pos_emb.log &


# 2025 10 03
nohup python train_t2m.py \
--exp-name exp_2025_10_03_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-rope-pos-emb \
--eval-masking \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_2025_10_03_t2m_transformer_vanilla_with_rptc_decoder_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_ver_rope_emb.log &

nohup python train_t2m.py \
--exp-name exp_2025_10_02_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_256_num_quantizer_6_drop_p_ver_2_no_corrupt_on_pad_end_token \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--dont-corrupt-pad-end \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_2025_10_02_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_rptc_ver_res_drop_with_soft_quantizer_cb_size_256_num_quantizer_6_drop_p_ver_2_no_corrupt_on_pad_end_token.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_10_02_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_force_drop_residual \
--use-keywords \
--force-drop-residual-quantization \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--output-emb-width 392 > log_eval_2025_10_02_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_force_drop_residual.log &



nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_10_02_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_force_drop_residual \
--use-keywords \
--force-drop-residual-quantization \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--output-emb-width 392 > log_eval_2025_10_02_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_force_drop_residual.log &

# 2025 10 01
nohup python train_rt2m.py \
--exp-name exp_2025_10_01_rt2m_transformer_vanilla_no_corrupt_data_renew \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_01 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_01 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_01_rt2m_transformer_vanilla_no_corrupt_data_renew.log &

nohup python train_rt2m.py \
--exp-name TEST_exp_2025_10_01_rt2m_transformer_vanilla_no_corrupt \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_09_30 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_09_30 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > TEST_log_exp_2025_10_01_rt2m_transformer_vanilla_no_corrupt.log &



# 2025 09 30
nohup python train_t2m.py \
--exp-name exp_2025_09_30_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_rptc_ver_res_drop_detach_p_latent_with_soft_quantizer_cb_size_256_num_quantizer_6_drop_p_ver_2_no_corrupt_on_pad_end_token \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--dont-corrupt-pad-end \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_2025_09_30_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_rptc_ver_res_drop_with_soft_quantizer_cb_size_256_num_quantizer_6_drop_p_ver_2_no_corrupt_on_pad_end_token.log &





nohup python train_rt2m.py \
--exp-name exp_2025_10_01_rt2m_transformer_vanilla_no_corrupt_data_renew \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_01_rt2m_transformer_vanilla_no_corrupt_data_renew.log &


# 2025 09 29
# 시범적
nohup python train_rt2m.py \
--exp-name exp_2025_09_30_rt2m_transformer_vanilla \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.1 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/pretrained/t2m/Trans \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_09_22 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_09_22 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_09_30_rt2m_transformer_vanilla.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6/2025-09-27_12-54-37 \
--output-emb-width 392 > log_eval_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-42 \
--output-emb-width 392 > log_eval_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6_drop_p_ver_2.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6/2025-09-27_12-54-44 \
--output-emb-width 392 > log_eval_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--output-emb-width 392 > log_eval_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2.log &


# 2025 09 27
nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 512 \
--rvq-mu 0.99 \
--detach-p-latent \
--pdrop-res 0.1 \
--rvq-commit 0.02 > log_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_ver_2.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 512 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--pdrop-res 0.1 \
--rvq-commit 0.02 > log_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 512 \
--rvq-mu 0.99 \
--detach-p-latent \
--pdrop-res 0.5 \
--rvq-commit 0.02 > log_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 512 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--pdrop-res 0.5 \
--rvq-commit 0.02 > log_2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6.log &



nohup python train_t2m.py \
--exp-name exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--resume-trans ./output/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-20_14-38-56/net_best_fid.pth \
--log-cat-right-num \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume.log &


# 2025 09 24

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent/2025-09-09_15-19-15 \
--output-emb-width 392 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_128 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_128/2025-09-16_03-09-28 \
--output-emb-width 392 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_128.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256/2025-09-17_00-47-40 \
--output-emb-width 392 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6/2025-09-18_08-05-36 \
--output-emb-width 392 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema/2025-09-15_00-18-34 \
--output-emb-width 392 > log_eval_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_128 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_128/2025-09-16_03-08-00 \
--output-emb-width 392 > log_eval_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_128.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6/2025-09-18_08-05-43 \
--output-emb-width 392 > log_eval_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6.log &




# 2025 09 22
nohup python train_rt2m.py \
--exp-name exp_2025_09_22_rt2m_transformer_vanilla \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.1 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/pretrained/t2m/Trans \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_09_22 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_09_22 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_09_22_rt2m_transformer_vanilla.log &

nohup python train_rt2m.py \
--exp-name exp_2025_09_22_rt2m_transformer_vanilla_no_keywords \
--batch-size 64 \
--num-layers 18 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 52 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.1 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_09_22 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_09_22 \
--t2m-checkpoint-folder /data/CoMo/pretrained/t2m/Trans \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 > log_exp_2025_09_22_rt2m_transformer_vanilla_no_keywords.log &

# 63 - 11 = 52 = q_emb(), text_emb, pose codes(=50)

# 주의 num keywords 없음

# 2025 09 19
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_20_Dec_vanilla \
--use-keywords \
--output-emb-width 392 
> log_2025_09_20_Dec_vanilla.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_zero \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--pdrop-res 0.0 \
--rvq-commit 0.02 > log_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_zero.log &

nohup python train_t2m.py \
--exp-name exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6/2025-09-19_02-40-52 \
--output-emb-width 392 > log_eval_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-35 \
--output-emb-width 392 > log_eval_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_ver_2.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6/2025-09-19_02-40-58 \
--output-emb-width 392 > log_eval_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--output-emb-width 392 > log_eval_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2.log &


nohup python train_rt2m.py \
--exp-name exp_2025_09_20_rt2m_transformer_vanilla \
--batch-size 64 \
--num-layers 12 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 63 \
--ff-rate 4 \
--out-dir output_rt2m \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--t2m-checkpoint-folder /data/CoMo/pretrained/t2m/Trans \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_09_20_rt2m_transformer_vanilla.log &

--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_09_18 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_09_18 \

# block size를 63로 하는 이유는 q_id emb가 추가되기 때문

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_zero \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--pdrop-res 0.1 \
--rvq-commit 0.02 > log_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6_drop_p_zero.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--pdrop-res 0.1 \
--rvq-commit 0.02 > log_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--pdrop-res 0.5 \
--rvq-commit 0.02 > log_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_256_num_quantizer_6.log &

nohup python train_rptc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--pdrop-res 0.5 \
--rvq-commit 0.02 > log_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6.log &


# 2025 09 18
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--rvq-commit 0.02 > log_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6.log &

nohup python train_t2m.py \
--exp-name exp_2025_09_18_t2m_transformer_vanilla_with_rptc_decoder_2025_09_15_rptc_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema/2025-09-15_00-18-34 \
--use-keywords > log_2025_09_18_t2m_transformer_vanilla_with_rptc_decoder_2025_09_15_rptc_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema.log &


# 2025 09 17
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_init_uniform \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--rvq-init-method uniform \
--rvq-commit 0.02 > log_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_init_uniform.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_256_num_quantizer_6.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 256 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--rvq-commit 0.02 > log_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6.log &

# 2025 09 16
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_128 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 128 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_cb_size_128.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_128 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 128 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--rvq-commit 0.02 > log_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_128.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema \
--resume-pth ./my_exp_result/2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema/2025-09-15_00-18-34/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--rvq-commit 0.02 > log_eval_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema.log &


# 2025 09 15
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_16_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_14_unuse_ema \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.0125 \
--unuse-ema \
--rvq-commit 0.02 > log_2025_09_16_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_14_unuse_ema.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_11 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.02 \
--rvq-commit 0.02 > log_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_11.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.01 \
--rvq-vq-loss-beta 0.25 \
--unuse-ema \
--rvq-commit 0.02 > log_2025_09_15_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_12_unuse_ema.log &

# 2025 09 14
nohup python train_t2m.py \
--exp-name exp_2025_09_14_t2m_transformer_vanilla_with_t2m_recons_feedback_bce_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v8.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_exp_2025_09_14_t2m_transformer_vanilla_with_t2m_recons_feedback_bce_loss.log &

nohup python train_t2m.py \
--exp-name exp_2025_09_12_t2m_transformer_vanilla_with_t2m_recons_feedback_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v7.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-trans ./output/exp_2025_09_12_t2m_transformer_vanilla_with_t2m_recons_feedback/2025-09-12_12-24-13/net_last.pth \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_exp_2025_09_12_t2m_transformer_vanilla_with_t2m_recons_feedback_resume.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_9 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.02 \
--rvq-vq-loss-beta 0.25 \
--rvq-commit 0.05 > log_2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_9.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v5.yaml \
--exp-name 2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_10 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--params-soft-ent-loss 0.02 \
--rvq-vq-loss-beta 0.5 \
--rvq-commit 0.05 > log_2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_10.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--warm-up-iter 1000 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_5 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--rvq-commit 0.04 > log_2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_5.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--rvq-commit 0.02 \
--warm-up-iter 5000 \
--params-soft-ent-loss 0.05 > log_2025_09_14_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer_try_6.log &

# 2025 09 13

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_09_13_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-quantizer-type soft \
--rvq-commit 0.02 > log_2025_09_13_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_soft_quantizer.log &

## 2025 09 11

nohup python train_t2m.py \
--exp-name exp_2025_09_12_t2m_transformer_vanilla_with_t2m_recons_feedback \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v7.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_exp_2025_09_12_t2m_transformer_vanilla_with_t2m_recons_feedback.log &


# -> learnable embedding
nohup python train_t2m.py \
--exp-name exp_2025_09_11_t2m_transformer_vanilla_with_silu_act \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--resume-trans ./output/exp_2025_09_09_t2m_transformer_vanilla/2025-09-08_15-25-40/net_last.pth \
--use-keywords > log_2025_09_11_exp_t2m_transformer_vanilla_resume.log &

--ffn-act-drop-out-rate \
--ffn-use-gated \
--ffn-subln \
--ffn-rmsnorm \
--ffn-elementwise \
--activation-fn-name \

# -> learnable embedding
nohup python train_t2m.py \
--exp-name exp_2025_09_11_t2m_transformer_with_learnable_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--learnable-pos-emb \
--resume-trans ./output/exp_2025_09_09_t2m_transformer_vanilla/2025-09-08_15-25-40/net_last.pth \
--use-keywords > log_2025_09_11_t2m_transformer_with_silu_act.log &


# -> activation = silu
nohup python train_t2m_v2.py \
--exp-name exp_2025_09_11_t2m_transformer_vanilla_with_swish_GLU \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--ffn-act-drop-out-rate 0.1 \
--ffn-use-gated \
--ffn-subln \
--ffn-rmsnorm \
--ffn-elementwise \
--activation-fn-name silu \
--use-keywords > log_2025_09_11_t2m_transformer_vanilla_with_swish_GLU.log &




nohup python train_t2m.py \
--exp-name exp_2025_09_11_t2m_transformer_vanilla_with_learnable_pos_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--resume-trans ./output/exp_2025_09_09_t2m_transformer_vanilla/2025-09-08_15-25-40/net_last.pth \
--use-keywords > log_2025_09_11_exp_t2m_transformer_vanilla_resume.log &


###
nohup python train_t2m_with_soft_label_with_moe.py \
--exp-name exp_2025_09_06_t2m_transformer_moe_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 200000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/logits" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--tau-max 0.8 \
--tau-min 0.8 \
--use-moe \
--moe-top-k 2 \
--moe-expert-count 32 \
--moe-elementwise \
--moe-rms-norm \
--moe-subln \
--moe-use-gated-fn \
--batch-prioritized-routing \
--w-load-balance 0.1 \
--use-keywords > log_2025_09_06_t2m_transformer_moe_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8.log &


nohup python train_t2m.py \
--exp-name exp_2025_09_11_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_warm_up_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--resume-trans ./output/exp_2025_09_09_t2m_transformer_cace_kl_loss_with_eval_masking_with_warm_up/2025-09-08_15-25-27/net_last.pth \
--use-keywords > log_2025_09_11_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_warm_up_resume.log &

# 2025 09 10
nohup python train_t2m_masking_ver_1.py \
--exp-name exp_2025_09_11_t2m_transformer_cace_kl_loss_with_eval_masking_with_warm_up_new_masking_ver_1_3 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--start-warm-up \
--num_workers 6 \
--min-sampling-prob 0.1 \
--alpha-max 0.9 \
--alpha-min 0.5 \
--beta 0.1 0.3 0.5 0.7 0.9 \
--mask-fast \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_09_11_t2m_transformer_cace_kl_loss_with_eval_masking_with_warm_up_new_masking_ver_1_3.log &

#--scheduled-sampling \ <- 뺐음


# 2025 09 09


# Decoder + RVQ 기반으로 훈련시키는 것 필요
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_rvq_sharing_for_log_perplexity \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-shared-codebook \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_rvq_sharing_for_log_perplexity.log &


# Baseline Decoder
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v1.yaml \
--exp-name 2025_06_17_Dec_vanilla \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_Dec_vanilla.log &

nohup python train_t2m_with_soft_label.py \
--exp-name exp_2025_09_09_t2m_transformer_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--num_workers 6 \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/logits" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--tau-max 0.8 \
--tau-min 0.8 \
--eval-masking \
--start-warm-up \
--min-sampling-prob 0.5 \
--scheduled-sampling \
--use-keywords > log_2025_09_09_t2m_transformer_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8.log &

# Baseline T2M
nohup python train_t2m.py \
--exp-name exp_2025_09_11_t2m_transformer_vanilla_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 6 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--resume-trans ./output/exp_2025_09_09_t2m_transformer_vanilla/2025-09-08_15-25-40/net_last.pth \
--use-keywords > log_2025_09_11_exp_t2m_transformer_vanilla_resume.log &

# Baseline T2M
nohup python train_t2m.py \
--exp-name exp_2025_09_09_t2m_transformer_cace_kl_loss_with_eval_masking_with_warm_up \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--start-warm-up \
--min-sampling-prob 0.5 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--scheduled-sampling \
--use-keywords > log_2025_09_08_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_warm_up.log &


# 2025 09 08
nohup python train_t2m.py \
--exp-name exp_2025_09_11_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_warm_up_resume \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--start-warm-up \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--resume-trans ./output/exp_2025_09_09_t2m_transformer_cace_kl_loss_with_eval_masking_with_warm_up/2025-09-08_15-25-27/net_last.pth \
--use-keywords > log_2025_09_11_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_warm_up_resume.log &


# 2025 09 07
nohup python train_t2m_with_self_pred_labeling_v1.py \
--exp-name exp_2025_09_07_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_self_pred_label_half_self_pred \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--fkeep-min 0.7 \
--fkeep-max 0.5 \
--prob-to-noise 0.5 \
--use-keywords > log_2025_09_07_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking_with_self_pred_label_half_self_pred.log &

# 70% -> 50%

nohup python train_t2m.py \
--exp-name exp_2025_09_07_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--eval-masking \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_09_07_t2m_transformer_vanilla_code_refined_ver_cace_loss_with_eval_masking.log &


# 2025 09 06
nohup python train_t2m_with_soft_label_with_moe.py \
--exp-name exp_2025_09_06_t2m_transformer_moe_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 200000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/logits" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--tau-max 0.8 \
--tau-min 0.8 \
--use-moe \
--moe-top-k 2 \
--moe-expert-count 32 \
--moe-elementwise \
--moe-rms-norm \
--moe-subln \
--moe-use-gated-fn \
--batch-prioritized-routing \
--w-load-balance 0.1 \
--use-keywords > log_2025_09_06_t2m_transformer_moe_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8.log &

# 2025 09 05

nohup python train_t2m_with_soft_label.py \
--exp-name exp_2025_09_05_t2m_transformer_vanilla_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 200000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v6.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/logits" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--tau-max 0.8 \
--tau-min 0.8 \
--use-keywords > log_2025_09_05_t2m_transformer_vanilla_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_fixed_temp_0p8.log &


# 2025 09 03
nohup python train_t2m_with_soft_label.py \
--exp-name exp_2025_09_03_t2m_transformer_vanilla_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_adaptive_temperature_add_unimodal_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 200000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v5.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/logits" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--tau-max 0.8 \
--tau-min 0.1 \
--use-keywords > log_2025_09_02_t2m_transformer_vanilla_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_adaptive_temperature_add_unimodal_loss.log &


# 2025 09 02
nohup python train_t2m_with_soft_label.py \
--exp-name exp_2025_09_02_t2m_transformer_vanilla_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_adaptive_temperature \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 200000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v4.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/logits" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--tau-max 0.8 \
--tau-min 0.1 \
--use-keywords > log_2025_09_02_t2m_transformer_vanilla_with_soft_labeling_based_symetric_descending_cace_kl_loss_temp_with_soft_masking_adaptive_temperature.log &


nohup python train_t2m_temp.py \
--exp-name exp_2025_09_02_t2m_transformer_vanilla_code_refined_ver_with_soft_labeling_based_symetric_descending_tau_0_3_cace_kl_loss_temp_with_no_masking \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v4.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/tau_0.3" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_09_02_t2m_transformer_vanilla_code_refined_ver_with_soft_based_symetric_descending_labeling_tau_0_3_cace_kl_loss_temp_with_no_masking.log &


nohup python train_t2m.py \
--exp-name exp_2025_09_02_t2m_transformer_vanilla_code_refined_ver_with_soft_labeling_based_symetric_descending_tau_0_3 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--soft-label-folder-name "soft_labeling_based_symetric_descending/tau_0.3" \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_09_02_t2m_transformer_vanilla_code_refined_ver_with_soft_labeling_tau_0_3.log &

# 2025 09 01
nohup python train_t2m.py \
--exp-name exp_2025_09_01_t2m_transformer_vanilla_code_refined_ver_with_soft_labeling \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--enable-soft-label \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_09_01_t2m_transformer_vanilla_code_refined_ver_with_soft_labeling.log &



nohup python train_t2m.py \
--exp-name exp_2025_09_01_t2m_transformer_vanilla_code_refined_ver_with_unimodal_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v3.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_09_01_t2m_transformer_vanilla_code_refined_ver_with_unimodal_loss.log &

nohup python train_t2m.py \
--exp-name TEST_exp_2025_09_08_t2m_transformer_vanilla_code_refined_ver \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 200 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v3.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > TEST_exp_2025_09_08_t2m_transformer_vanilla_code_refined_ver.log &

# 2025 08 29
nohup python train_t2m.py \
--exp-name exp_2025_08_29_t2m_transformer_vanilla_code_refined_ver_cace_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_29_train_t2m_transformer_code_refined_ver_cace_loss.log &

nohup python train_t2m.py \
--exp-name exp_2025_08_29_t2m_transformer_vanilla_code_refined_ver_with_no_duplicated_pos_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--disable-pos-emb-additional \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_29_train_t2m_transformer_code_refined_ver_with_no_duplicated_pos_emb.log &

nohup python train_t2m.py \
--exp-name exp_2025_08_29_t2m_transformer_vanilla_code_refined_ver_group_aware_tok_embedding_v2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--token-emb-layer group-aware-v2 \
--use-keywords > log_2025_08_29_train_t2m_transformer_code_refined_ver_group_aware_tok_embedding_v2.log &

nohup python train_t2m.py \
--exp-name exp_2025_08_29_t2m_transformer_vanilla_code_refined_ver \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_29_train_t2m_transformer_code_refined_ver.log &


# 2025 08 28
nohup python train_t2m.py \
--exp-name exp_2025_08_28_t2m_transformer_vanilla_code_refined_ver_group_aware_tok_embedding_v2_with_cace_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--token-emb-layer group-aware-v2 \
--use-keywords > log_2025_08_28_train_t2m_transformer_code_refined_ver_group_aware_tok_embedding_v2_with_cace_loss.log &

nohup python train_t2m.py \
--exp-name exp_2025_08_28_t2m_transformer_vanilla_code_refined_ver \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_28_train_t2m_transformer_code_refined_ver.log &

nohup python train_t2m.py \
--exp-name exp_2025_08_28_t2m_transformer_vanilla_code_refined_ver_with_no_duplicated_pos_emb \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--disable-pos-emb-additional \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_28_train_t2m_transformer_code_refined_ver_with_no_duplicated_pos_emb.log &


nohup python train_t2m.py \
--exp-name exp_2025_08_28_t2m_transformer_vanilla_code_refined_ver_group_aware_tok_embedding_v2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--token-emb-layer group-aware-v2 \
--use-keywords > log_2025_08_28_train_t2m_transformer_code_refined_ver_group_aware_tok_embedding_v2.log &


nohup python train_t2m.py \
--exp-name exp_2025_08_28_t2m_transformer_vanilla_code_refined_ver_cace_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_28_train_t2m_transformer_code_refined_ver_cace_loss.log &


# 2025 08 27
nohup python train_t2m.py \
--exp-name exp_2025_08_27_t2m_transformer_vanilla_code_refined_ver_updated_group_aware_tok_embedding_v2_with_cace_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--token-emb-layer group-aware-v2 \
--use-keywords > log_2025_08_27_train_t2m_transformer_code_refined_ver_updated_group_aware_tok_embedding_v2_with_cace_loss.log &



# 2025-08-26
nohup python train_t2m.py \
--exp-name exp_2025_08_26_t2m_transformer_vanilla_code_refined_ver_updated_eval_mode_on_validation_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_26_train_t2m_transformer_code_refined_ver_updated_eval_mode_on_validation_loss.log &


nohup python train_t2m.py \
--exp-name exp_2025_08_26_t2m_transformer_vanilla_code_refined_ver_updated_group_aware_tok_embedding_v2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--token-emb-layer group-aware-v2 \
--use-keywords > log_2025_08_26_train_t2m_transformer_code_refined_ver_updated_group_aware_tok_embedding_v2.log &


# 2025 08 25
nohup python train_t2m.py \
--exp-name TEST_exp_2025_09_01_t2m_transformer_vanilla_code_refined_ver \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 200 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v3.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > TEST_exp_2025_09_01_t2m_transformer_vanilla_code_refined_ver.log &


nohup python train_t2m.py \
--exp-name exp_2025_08_25_t2m_transformer_vanilla_code_refined_ver_cace_loss \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v2.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_25_train_t2m_transformer_code_refined_ver_cace_loss.log &

nohup python train_t2m.py \
--exp-name exp_2025_08_25_t2m_transformer_vanilla_code_refined_ver \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m/losses_v1.yaml \
--log-cat-right-num \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_08_25_train_t2m_transformer_code_refined_ver.log &


# 2025 08 22
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_22_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_2_rvq_cb_num_128 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 128 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > exp_log_2025_08_22_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_2_rvq_cb_num_128.log &

### q layer 6 + not sharing
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_q_layer_6 \
--resume-pth ./my_exp_result/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_q_layer_6/2025-08-21_04-21-52/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--detach-p-latent \
--rvq-mu 0.99 > log_eval_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_q_layer_6.log &


### q layer 6 + cb num 128
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6_rvq_cb_num_128 \
--resume-pth ./my_exp_result/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6_rvq_cb_num_128/2025-08-21_03-48-22/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 128 \
--detach-p-latent \
--rvq-mu 0.99 > log_eval_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6_rvq_cb_num_128.log &


# 2025 08 21
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_q_layer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > exp_log_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_q_layer_6.log &

### q layer 6 + codebook 수 늘리기(64 -> 128)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6_rvq_cb_num_128 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 128 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > exp_log_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6_rvq_cb_num_128.log &

### q layer 6
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6 \
--resume-pth ./my_exp_result/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6/2025-08-20_14-19-53/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--detach-p-latent \
--rvq-mu 0.99 > log_eval_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6.log &


### q layer 4
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_4 \
--resume-pth ./my_exp_result/2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_4/2025-08-20_14-19-47/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 4 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--detach-p-latent \
--rvq-mu 0.99 > log_eval_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_4.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 6 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > exp_log_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_6.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_4 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 4 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > exp_log_2025_08_21_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_q_layer_4.log &

# 2025 08 18


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 361 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v7 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v4.yaml \
--exp-name 2025_08_18_Dec_vanilla_with_new_pose_code_system_v7 \
--use-keywords \
--output-emb-width 361 
> log_2025_08_18_Dec_vanilla_with_new_pose_code_system_v7.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v6_v3 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_18_Dec_vanilla_with_new_pose_code_system_v6_v3 \
--use-keywords \
--output-emb-width 392 
> log_2025_08_18_Dec_vanilla_with_new_pose_code_system_v6_v3.log &

# 2025 08 13
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor \
--resume-pth ./my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor/2025-08-04_05-42-19/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor.log &


# 2025 08 12
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_07_29_contrastive/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq_with_contrastive_loss_ver_2 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_sharing_rvq_with_contrastive_loss_ver_2.log &

# 2025-08-11
nohup python train_t2m_with_rvq.py \
--exp-name exp_2025_08_11_rvq_t2m_transformer \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--use-keywords > log_2025_08_11_rvq_t2m_transformer.log &



####

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq \
--resume-pth ./my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq/2025-08-07_03-32-27/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--detach-p-latent \
--rvq-mu 0.99 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook \
--resume-pth ./my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook/2025-08-04_05-43-10/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook.log &


# 2025 08 09
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/exp_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent \
--resume-pth ./my_exp_result/2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent/2025-08-04_07-19-32/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent > log_eval_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent.log &




# 2025 08 06
## rvq sharing + p latent detach 버전 해야함
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_with_sharing_rvq \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-shared-codebook \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent_sharing_rvq.log &

## 더 앞으로 진행하게 된다면 Transformer 구조까지도 생각 필요함


# 2025 08 04
# 
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--detach-p-latent \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_ver_detach_p_latent.log &


# orthogonal loss 추가
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v2.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook_add_orthogonal_loss \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-shared-codebook \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 \
--rvq-resi-beta 1.0 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook_add_orthogonal_loss.log &


# codebook을 sharing 하지 않는 경우
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-shared-codebook \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 \
--rvq-resi-beta 1.0 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_sharing_codebook.log &

# beta, commit 모두 0으로 둘경우
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_resi_beta_ver_4 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.0 \
--rvq-resi-beta 0.0 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_resi_beta_ver_4.log &

# beta 모두 0으로 둘경우
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_resi_beta_ver_3 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 \
--rvq-resi-beta 0.0 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_resi_beta_ver_3.log &

# beta, commit 모두 활성화, beta 줄이기
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_resi_beta_ver_2 \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 \
--rvq-resi-beta 0.5 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor_rvq_resi_beta_ver_2.log &

# beta, commit 모두 활성화(기본)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_08_04_Dec_ver_using_residual_pose_temporal_complementor \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-name rptc \
--rvq-num-quantizers 2 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 > log_2025_08_04_Dec_ver_using_residual_pose_temporal_complementor.log &



# 2025 08 03
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_07_30/losses_v1.yaml \
--exp-name 2025_08_03_Dec_vanilla_with_randomize_window_size \
--use-keywords \
--randomize-window-size \
--output-emb-width 392 > log_2025_08_03_Dec_vanilla_with_randomize_window_size.log &

# 
nohup python train_t2m.py \
--exp-name exp_2025_08_03_t2m_transformer_vanilla_with_new_pose_code_system_v2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 568 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--codes-folder-name codes_v2 \
--dilation-growth-rate 3 \
--output-emb-width 568 \
--val-shuffle \
--resume-pth ./my_exp_result/2025_07_14_Dec_vanilla_with_new_pose_code_system_v2/2025-07-14_01-45-53/net_best_fid.pth \
--cat-mode v2 \
--use-keywords > log_2025_08_03_train_t2m_transformer_vanilla_with_new_pose_code_system_v2.log &

nohup python eval_t2m.py \
--exp-name TEST_exp_2025_10_15_t2m_transformer_ver_focal_bce_loss_ver_conf_based_symmetric_masking_ver_random_schedule \
--batch-size 32 \
--num-layers 9 \
--embed-dim-gpt 1024 \
--nb-code 412 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--resume-pth ./my_exp_result/2025_07_24_Dec_vanilla_with_new_pose_code_system_v4/2025-07-23_15-22-49/net_best_fid.pth \
--vq-name VQVAE_2025_08_03_Dec_vanilla_with_new_pose_code_system_v4_re \
--out-dir eval_output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--vq-act relu \
--codes-folder-name codes_v4 \
--output-emb-width 412 \
--resume-trans ./output/exp_2025_07_25_t2m_transformer_vanilla_with_new_pose_code_system_v4/2025-07-25_13-49-31/net_best_fid.pth \
--use-keywords \
--cat-mode v4 > log_2025_08_03_eval_t2m_transformer_vanilla_with_new_pose_code_system_v4_re.log &

nohup python eval_t2m.py \
--exp-name TEST_exp_2025_08_02_t2m_vanilla_transformer \
--batch-size 32 \
--num-layers 9 \
--embed-dim-gpt 1024 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--vq-name VQVAE_pretrained \
--out-dir eval_output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--vq-act relu \
--output-emb-width 392 \
--resume-trans ./output/exp_2025_07_15_t2m_transformer_vanilla/2025-07-15_05-24-46/net_best_fid.pth \
--use-keywords > log_2025_08_02_eval_t2m_transformer_vanilla.log &

# 2025-07-30
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--loss-cfg-path /data/CoMo/configs/exp_2025_07_30/losses_v1.yaml \
--exp-name 2025_07_30_Dec_vanilla_with_new_pose_code_system_v2_with_velocity_loss_ver_3 \
--use-keywords \
--output-emb-width 568 > log_2025_07_30_Dec_vanilla_with_new_pose_code_system_v2_with_velocity_loss_ver_3.log &

# 2025-07-30
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v2.yaml \
--exp-name 2025_07_30_Dec_vanilla_with_new_pose_code_system_v2_with_orthogonality_loss \
--use-keywords \
--output-emb-width 568 
> log_2025_07_30_Dec_vanilla_with_new_pose_code_system_v2.log &

# 2025-07-29
# orthogonality loss + code v2
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v2.yaml \
--exp-name 2025_07_29_Dec_vanilla_with_new_pose_code_system_v2_with_orthogonality_loss \
--use-keywords \
--output-emb-width 568 
> log_2025_07_29_Dec_vanilla_with_new_pose_code_system_v2.log &


##
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_07_29_contrastive/losses_v2.yaml \
--exp-name 2025_07_29_Dec_ver_code_contrastive_loss_v2 \
--use-keywords \
--output-emb-width 392 > log_2025_07_29_Dec_ver_code_contrastive_loss_v2.log & 

## 
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_07_29_contrastive/losses_v1.yaml \
--exp-name 2025_07_29_Dec_ver_code_contrastive_loss_v1 \
--use-keywords \
--output-emb-width 392 > log_2025_07_29_Dec_ver_code_contrastive_loss_v1.log &


## entropy 낮은 case들 제거 후 훈련
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_29_Dec_ver_code_delete_super_posecode_low_entropy_custom_2025_07_29_low_entropy_v1 \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "custom_2025_07_29_low_entropy_v1" 
> log_2025_07_29_Dec_ver_code_delete_super_posecode_low_entropy_custom_2025_07_29_low_entropy_v1.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_29_Dec_ver_code_delete_super_posecode_low_entropy_custom_2025_07_29_low_entropy_v2 \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "custom_2025_07_29_low_entropy_v2" 
> log_2025_07_29_Dec_ver_code_delete_super_posecode_low_entropy_custom_2025_07_29_low_entropy_v2.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_27_Dec_ver_code_delete_super_posecode_relpos \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "relpos" 
> log_2025_07_27_Dec_ver_code_delete_super_posecode_relpos.log &

# 2025-07-28
## eval 다시 하기
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--exp-name TEST_Dec/exp_2025_07_14_Dec_vanilla_with_new_pose_code_system_v2 \
--resume-pth ./my_exp_result/2025_07_14_Dec_vanilla_with_new_pose_code_system_v2/2025-07-14_01-45-53/net_best_fid.pth \
--use-keywords \
--output-emb-width 568 
> log_2025_07_14_eval_dec_exp_Dec_vanilla_with_new_pose_code_system_v2.log &

nohup python eval_t2m.py \
--exp-name TEST_exp_2025_07_25_t2m_transformer_vanilla_with_new_pose_code_system_v4 \
--batch-size 32 \
--num-layers 9 \
--embed-dim-gpt 1024 \
--nb-code 412 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--resume-pth ./my_exp_result/2025_07_24_Dec_vanilla_with_new_pose_code_system_v4/2025-07-23_15-22-49/net_best_fid.pth \
--vq-name VQVAE_2025_07_24_Dec_vanilla_with_new_pose_code_system_v4 \
--out-dir eval_output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--vq-act relu \
--output-emb-width 412 \
--resume-trans ./output/exp_2025_07_25_t2m_transformer_vanilla_with_new_pose_code_system_v4/2025-07-25_13-49-31/net_best_fid.pth \
--use-keywords \
--cat-mode v4 > log_2025_07_25_eval_t2m_transformer_vanilla_with_new_pose_code_system_v4.log &

# rvq
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_28_Dec_ver_using_rvq \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-num-quantizers 3 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 > log_2025_07_28_Dec_ver_using_rvq.log &

# rvq sharing
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_28_Dec_ver_using_rvq_sharing_codebook \
--use-keywords \
--output-emb-width 392 \
--use-rvq \
--rvq-num-quantizers 3 \
--rvq-quantize-dropout-prob 0.2 \
--rvq-quantize-dropout-cutoff-index 0 \
--rvq-shared-codebook \
--rvq-nb-code 64 \
--rvq-mu 0.99 \
--rvq-commit 0.02 > log_2025_07_28_Dec_ver_using_rvq_sharing_codebook.log &
#


# 2025-07-27
# relpos
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_27_Dec_ver_code_delete_super_posecode_relpos \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "relpos" 
> log_2025_07_27_Dec_ver_code_delete_super_posecode_relpos.log &

# relV
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_27_Dec_ver_code_delete_super_posecode_relV \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "relV" 
> log_2025_07_27_Dec_ver_code_delete_super_posecode_relV.log &

# v6 = rotation 계산 수정
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v6 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_27_Dec_vanilla_with_new_pose_code_system_v6 \
--use-keywords \
--output-emb-width 392 
> log_2025_07_27_Dec_vanilla_with_new_pose_code_system_v6.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v6_v2 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_27_Dec_vanilla_with_new_pose_code_system_v6_v2 \
--use-keywords \
--output-emb-width 392 
> log_2025_07_27_Dec_vanilla_with_new_pose_code_system_v6_v2.log &


# 2025-07-26
nohup python train_t2m.py \
--exp-name exp_2025_07_25_t2m_transformer_vanilla_with_new_pose_code_system_v4 \
--batch-size 64 \
--num-layers 9 \
--nb-code 412 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--codes-folder-name codes_v4 \
--dilation-growth-rate 3 \
--output-emb-width 412 \
--val-shuffle \
--resume-pth ./my_exp_result/2025_07_24_Dec_vanilla_with_new_pose_code_system_v4/2025-07-23_15-22-49/net_best_fid.pth \
--cat-mode v4 \
--use-keywords > log_2025_07_25_train_t2m_transformer_vanilla_with_new_pose_code_system_v4.log &



# 2025-07-24
1. only angle, distance category -> t2m generator training
nohup python train_t2m.py \
--exp-name exp_2025_07_25_t2m_transformer_vanilla_with_new_pose_code_system_v5 \
--batch-size 64 \
--num-layers 9 \
--nb-code 252 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--codes-folder-name codes_v5 \
--dilation-growth-rate 3 \
--output-emb-width 252 \
--val-shuffle \
--resume-pth ./my_exp_result/2025_07_24_Dec_vanilla_with_new_pose_code_system_v5/2025-07-24_09-26-43/net_best_fid.pth \
--cat-mode v5 \
--use-keywords > log_2025_07_25_train_t2m_transformer_vanilla_with_new_pose_code_system_v5.log &

2. summation 대신 hadamard prod

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_25_Dec_vanilla_with_agg_hadamard \
--use-keywords \
--aggregate-mode hadamard \
--output-emb-width 392 
> log_2025_07_25_Dec_vanilla_with_agg_hadamard.log &


# 2025-07-24
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 252 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v5 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_24_Dec_vanilla_with_new_pose_code_system_v5 \
--use-keywords \
--output-emb-width 252 
> log_2025_07_24_Dec_vanilla_with_new_pose_code_system_v5.log &



# 2025-07-23

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 412 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v4 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v3.yaml \
--exp-name 2025_07_24_Dec_vanilla_with_new_pose_code_system_v4_velocity_loss_v3 \
--use-keywords \
--output-emb-width 412 
> log_2025_07_24_Dec_vanilla_with_new_pose_code_system_v4_velocity_loss_v3.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 412 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v4 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_24_Dec_vanilla_with_new_pose_code_system_v4 \
--use-keywords \
--output-emb-width 412 
> log_2025_07_24_Dec_vanilla_with_new_pose_code_system_v4.log &

# 2025-07-15

nohup python train_t2m.py \
--exp-name exp_2025_07_15_t2m_transformer_vanilla_with_new_pose_code_system_v3 \
--batch-size 64 \
--num-layers 9 \
--nb-code 568 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--codes-folder-name codes_v3 \
--dilation-growth-rate 3 \
--output-emb-width 568 \
--val-shuffle \
--resume-pth ./my_exp_result/2025_07_14_Dec_vanilla_with_new_pose_code_system_v3/2025-07-14_07-34-09/net_best_fid.pth \
--use-keywords > log_2025_07_17_train_t2m_transformer_vanilla_with_new_pose_code_system_v3.log &


nohup python train_t2m.py \
--exp-name exp_2025_07_15_t2m_transformer_vanilla_with_new_pose_code_system_v2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 568 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--codes-folder-name codes_v2 \
--dilation-growth-rate 3 \
--output-emb-width 568 \
--val-shuffle \
--resume-pth ./my_exp_result/2025_07_14_Dec_vanilla_with_new_pose_code_system_v2/2025-07-14_01-45-53/net_best_fid.pth \
--use-keywords > log_2025_07_15_train_t2m_transformer_vanilla_with_new_pose_code_system_v2.log &

# Generator 학습

nohup python train_t2m.py \
--exp-name exp_2025_07_15_t2m_transformer_vanilla \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords > log_2025_07_15_train_t2m_transformer_vanilla.log &



# 2025-07-15

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v3 \
--exp-name TEST_Dec/exp_2025_07_13_Dec_vanilla_with_new_pose_code_system_v3 \
--resume-pth ./my_exp_result/2025_07_14_Dec_vanilla_with_new_pose_code_system_v3/2025-07-14_07-34-09/net_best_fid.pth \
--use-keywords \
--output-emb-width 568 
> log_2025_07_15_eval_dec_exp_Dec_vanilla_with_new_pose_code_system_v3.log &


## Motion Decoder
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--exp-name TEST_Dec/exp_2025_07_13_Dec_vanilla_with_new_pose_code_system_v2 \
--resume-pth ./my_exp_result/2025_07_13_Dec_vanilla_with_new_pose_code_system_v2/2025-07-14_01-30-33/net_best_fid.pth \
--use-keywords \
--output-emb-width 568 
> log_2025_07_15_eval_dec_exp_Dec_vanilla_with_new_pose_code_system_v2.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--exp-name TEST_Dec/exp_2025_07_13_Dec_vanilla_with_new_pose_code_system_v2_net_last \
--resume-pth ./my_exp_result/2025_07_13_Dec_vanilla_with_new_pose_code_system_v2/2025-07-14_01-30-33/net_last.pth \
--use-keywords \
--output-emb-width 568 
> log_2025_07_15_eval_dec_exp_Dec_vanilla_with_new_pose_code_system_v2_net_last.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/exp_2025_07_13_Dec_vanilla \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords \
--output-emb-width 392 
> log_2025_07_15_eval_dec_exp_Dec_vanilla.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--exp-name TEST_Dec/exp_2025_07_13_Dec_vanilla_code_refined_ver \
--resume-pth ./my_exp_result/2025_06_15_Dec_vanilla_code_refined_ver/2025-06-15_13-47-43/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 
> log_2025_07_15_eval_dec_exp_Dec_vanilla_code_refined_ver.log &

# 2025-07-14
pose code v3 - relative pos xyz -> rotation invariant coordinate로 계산
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v3 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_14_Dec_vanilla_with_new_pose_code_system_v3 \
--use-keywords \
--output-emb-width 568 
> log_2025_07_14_Dec_vanilla_with_new_pose_code_system_v3.log &


# 2025-07-14
pose code v2 - 그룹 세분화(angle, distance는 고정)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 250000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_14_Dec_vanilla_with_new_pose_code_system_v2 \
--use-keywords \
--output-emb-width 568 
> log_2025_07_14_Dec_vanilla_with_new_pose_code_system_v2_resume.log &


# 2025-07-13
-> 분포 재수정 후 새로운 pose code로 실험
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 568 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--codes-folder-name codes_v2 \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_13_Dec_vanilla_with_new_pose_code_system_v2 \
--use-keywords \
--output-emb-width 392 
> log_2025_07_13_Dec_vanilla_with_new_pose_code_system_v2.log &


-> 중요도 낮은 pose code 제거 실험

## 1. pelvis_vs_neck_relV
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_pelvis_vs_neck_relV \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "pelvis vs neck relV"
> log_2025_07_03_Dec_ver_code_delete_pelvis_vs_neck_relV.log &

## 1. L-hand_vs_R-hand_relY
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-hand_vs_R-hand_relY \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-hand vs R-hand relY"
> log_2025_07_03_Dec_ver_code_delete_L-hand_vs_R-hand_relY.log &

## 1. R-hand_vs_R-knee_distance
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-hand_vs_R-knee_distance \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-hand vs R-knee distance"
> log_2025_07_03_Dec_ver_code_delete_R-hand_vs_R-knee_distance.log &


## 1. R-wrist_vs_neck_relY
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-wrist_vs_neck_relY \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-wrist vs neck relY"
> log_2025_07_03_Dec_ver_code_delete_R-wrist_vs_neck_relY.log &

## 2. R-hand_vs_L-shoulder_distance
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-hand_vs_L-shoulder_distance \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-hand vs L-shoulder distance"
> log_2025_07_03_Dec_ver_code_delete_R-hand_vs_L-shoulder_distance.log &


## 3. L-hand_vs_R-shoulder_relY
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-hand_vs_R-shoulder_relY \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-hand vs R-shoulder relY"
> log_2025_07_03_Dec_ver_code_delete_L-hand_vs_R-shoulder_relY.log &

## 4. R-hand_vs_R-shoulder_relY
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-hand_vs_R-shoulder_relY \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-hand vs R-shoulder relY"
> log_2025_07_03_Dec_ver_code_delete_R-hand_vs_R-shoulder_relY.log &

## 5. L-knee_vs_R-knee_relY
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-knee_vs_R-knee_relY \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-knee vs R-knee relY"
> log_2025_07_03_Dec_ver_code_delete_L-knee_vs_R-knee_relY.log &

## 6. neck_vs_pelvis_relZ
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_neck_vs_pelvis_relZ \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "neck vs pelvis relZ"
> log_2025_07_03_Dec_ver_code_delete_neck_vs_pelvis_relZ.log &

## 7. L-knee_ground
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-knee_ground \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-knee ground"
> log_2025_07_03_Dec_ver_code_delete_L-knee_ground.laog &

#### 끝나고 더 돌려야 함
## 8. R-knee_ground
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-knee_ground \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-knee ground"
> log_2025_07_03_Dec_ver_code_delete_R-knee_ground.log &



# 2025-07-12

nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_L-elbow_angle/2025-07-05_14-47-18 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_L-elbow_angle.log &
nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_L-foot_vs_R-foot_distance/2025-07-05_14-47-06 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_L-foot_vs_R-foot_distance.log &
nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_L-hand_vs_L-knee_distance/2025-07-05_14-47-11 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_L-hand_vs_L-knee_distance.log &
nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_L-knee_angle/2025-07-05_14-46-48 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_L-knee_angle.log &

nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_R-elbow_angle/2025-07-05_14-47-23 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_R-elbow_angle.log &
nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_R-knee_angle/2025-07-05_14-46-57 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_R-knee_angle.log &
nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_super_posecode_angle/2025-07-06_01-37-03 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_super_posecode_angle.log &
nohup python exp_anl_pose_code_importance.py --ckpt-dir /data/CoMo/my_exp_result/2025_07_03_Dec_ver_code_delete_super_posecode_distance/2025-07-06_01-37-18 > log_2025_07_12_pose_code_importance_ablation_Dec_ver_code_delete_super_posecode_distance.log &





# 2025-07-07

nohup python exp_anl_pose_code_importance.py --mode ablation --ckpt-path > log_2025_07_12_pose_code_importance_ablation_L_hand_vs_R_hand_relX.log &


# 2025-07-05 - 상위 카테고리 제거(Super_posecode)
## Angle
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_super_posecode_angle \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "angle"
> log_2025_07_03_Dec_ver_code_delete_super_posecode_angle.log &

## distance
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_super_posecode_distance \
--use-keywords \
--output-emb-width 392 \
--delete-mode del_super_posecode \
--target-del-semantic-code "distance"
> log_2025_07_03_Dec_ver_code_delete_super_posecode_distance.log &


# 2025-07-05 - 단일 카테고리 제거
(WARNING: STEP 수 줄여놓음)

## 1. L-knee angle
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-knee_angle \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-knee angle" 
> log_2025_07_03_Dec_ver_code_delete_L-knee_angle.log &

## 2. R-knee angle
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-knee_angle \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-knee angle" 
> log_2025_07_03_Dec_ver_code_delete_R-knee_angle.log &

## 3. L-foot vs R-foot distance
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-foot_vs_R-foot_distance \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-foot vs R-foot distance" 
> log_2025_07_03_Dec_ver_code_delete_L-foot_vs_R-foot_distance.log &

## 4. L-hand vs L-knee distance
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-hand_vs_L-knee_distance \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-hand vs L-knee distance" 
> log_2025_07_03_Dec_ver_code_delete_L-hand_vs_L-knee_distance.log &

## 5. L-elbow angle
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_L-elbow_angle \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-elbow angle" 
> log_2025_07_03_Dec_ver_code_delete_L-elbow_angle.log &

## 6. R-elbow angle
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 200000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_07_03_Dec_ver_code_delete_R-elbow_angle \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "R-elbow angle" 
> log_2025_07_03_Dec_ver_code_delete_R-elbow_angle.log &


# 2025-06-24
nohup python exp_anl_pose_code_importance.py --mode ablation > log_2025_06_24_pose_code_importance_ablation_L_hand_vs_R_hand_relX.log &





nohup python exp_anl_motion_editing_v2.py > log_2025_06_24_motion_editing_v2.log &


# 2025-06-23

# target pose code 제거 후 실험(High Importance Code)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_06_23_Dec_ver_code_delete_L_hand_R_hand_rel_X_high_importance \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "L-hand vs R-hand relX" 
> log_2025_06_23_Dec_ver_code_delete_L_hand_R_hand_rel_X_high_importance.log &

## 392 -> 389
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 389 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_06_23_Dec_ver_code_trim_L_hand_R_hand_rel_X_high_importance_with_389_codes \
--use-keywords \
--output-emb-width 389 \
--delete-mode trim \
--target-del-semantic-code "L-hand vs R-hand relX" 
> log_2025_06_23_Dec_ver_code_trim_L_hand_R_hand_rel_X_high_importance_with_389_codes.log &


# target pose code 제거 후 실험(Low Importance Code)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_06_23_Dec_ver_code_delete_pelvis_neck_rel_V_low_importance \
--use-keywords \
--output-emb-width 392 \
--target-del-semantic-code "pelvis vs neck relV" 
> log_2025_06_23_Dec_ver_code_delete_pelvis_neck_rel_V_low_importance.log &

## 392 -> 389
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 389 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_06_23_Dec_ver_code_trim_pelvis_neck_rel_V_low_importance_with_389_codes \
--use-keywords \
--output-emb-width 389 \
--delete-mode trim \
--target-del-semantic-code "pelvis vs neck relV" 
> log_2025_06_23_Dec_ver_code_trim_pelvis_neck_rel_V_low_importance_with_389_codes.log &



# target pose code 제거 후 실험(Low Importance Code)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 391 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_23/losses_v1.yaml \
--exp-name 2025_06_23_Dec_ver_code_delete_pelvis_neck_rel_V_low_importance_with_391_codes \
--use-keywords \
--output-emb-width 391 \
--target-del-semantic-code "pelvis vs neck relV" 
> log_2025_06_23_Dec_ver_code_delete_pelvis_neck_rel_V_low_importance.log &



# 2025-06-22
nohup python exp_vis_motion_editing.py > log_2025_06_24_exp_vis_motion_editing.log &

# 2025-06-17

# pose code importance 측정
nohup python exp_anl_pose_code_importance_examine.py > log_2025_06_17_pose_code_importance_examine.log &
nohup python exp_anl_pose_code_importance.py --mode ablation > log_2025_06_17_pose_code_importance_ablation.log &
nohup python exp_anl_pose_code_importance.py --mode isolation > log_2025_06_17_pose_code_importance_isolation.log &





# 코드 수정후 다시
1. baseline 재현
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v1.yaml \
--exp-name 2025_06_17_Dec_vanilla_code_refined_ver \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_Dec_vanilla_code_refined_ver.log &

2. no velocity loss
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v2.yaml \
--exp-name 2025_06_17_Dec_ver_no_vel_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_Dec_ver_no_vel_loss.log &

3. recons loss main, logging joint-wise loss 
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v3.yaml \
--exp-name 2025_06_17_Dec_vanilla_logging_joint_wise_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_Dec_vanilla_logging_joint_wise_loss.log &

4. joint-wise loss
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v4.yaml \
--exp-name 2025_06_17_Dec_ver_joint_group_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_Dec_ver_joint_group_loss.log &



1. 원래 코드 다시 돌려보기
python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name Dec \
--output-emb-width 392


2. test 하여 velocity loss가 제대로 더해지는지 확인
python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v1.yaml \
--disable-align-root \
--exp-name Testing \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_testing.log


# 
nohup python exp_anl_pose_code_importance.py --mode ablation > log_2025_06_16_pose_code_importance_ablation.log &
nohup python exp_anl_pose_code_importance.py --mode isolation > log_2025_06_16_pose_code_importance_isolation.log & 

# Refined Trainnig code

python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/losses.yaml \
--exp-name Testing \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_Testing.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses.yaml \
--align-root \
--exp-name 2025_06_16_Dec_vanilla_code_refined_ver_align_root_joint \
--use-keywords \
--output-emb-width 392 
> log_2025_06_16_Dec_vanilla_code_refined_ver_align_root_joint.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v1.yaml \
--disable-align-root \
--exp-name 2025_06_16_Dec_vanilla_no_align_root_joint \
--use-keywords \
--output-emb-width 392 
> log_2025_06_16_Dec_vanilla_no_align_root_joint.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_16/losses_v2.yaml \
--disable-align-root \
--exp-name 2025_06_17_Dec_no_velocity_loss_no_align_root_joint \
--use-keywords \
--output-emb-width 392 
> log_2025_06_17_Dec_ver_no_velocity_loss_no_align_root_joint.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-cfg-path /data/CoMo/configs/exp_2025_06_15/losses.yaml \
--exp-name 2025_06_15_Dec_ver_joint_format_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_15_Dec_ver_joint_format_loss.log &


#--------- Training ---------#

## Motion Encoder[Optional]
python train_enc.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name Enc \
--output-emb-width 392 \
--use-keywords \
--resume-pth ./pretrained/t2m/Dec/model.pth

## Motion Decoder(HumanML3D)

# V1(Baseline)
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name 2025_05_18_Dec_vanilla_ver_5_with_add_loss_std \
--use-keywords \
--output-emb-width 392 
> train_dec_2025_05_18_vanilla_ver_5.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--vel-loss-mode v2 \
--recons-loss l1_smooth \
--exp-name 2025_05_24_Dec_vanilla_ver_6_with_fixed_velocity_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_05_24_Dec_vanilla_ver_6_with_fixed_velocity_loss.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--exp-name 2025_05_28_Dec_vanilla_ver_6_with_fixed_velocity_loss_ver_3 \
--use-keywords \
--output-emb-width 392 
> log_2025_05_28_Dec_vanilla_ver_6_with_fixed_velocity_loss_ver_3.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 1.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--exp-name 2025_05_28_Dec_vanilla_ver_fixed_velocity_loss_ver_3_inc_lambda_vel_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_02_Dec_vanilla_ver_fixed_velocity_loss_ver_3_inc_lambda_vel_loss.log &

# resume model training 
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--exp-name 2025_05_28_Dec_vanilla_ver_fixed_velocity_loss_ver_3 \
--use-keywords \
--output-emb-width 392 
> log_2025_06_02_Dec_vanilla_ver_fixed_velocity_loss_ver_3.log &

# resume model training 
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 1.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--exp-name 2025_05_18_Dec_vanilla_ver_5_add_loss_std_with_loss_orthogonal_with_param_ver_3 \
--use-keywords \
--output-emb-width 392 
> log_2025_06_02_Dec_vanilla_ver_fixed_velocity_loss_ver_3_inc_lambda_vel_loss.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--vel-loss-mode v1 \
--recons-loss l1_smooth \
--exp-name 2025_06_03_Dec_vanilla_ver_change_meta_file \
--meta-dir /data/HumanML3D/HumanML3D/meta \
--use-keywords \
--output-emb-width 392 
> log_2025_06_03_Dec_vanilla_ver_change_meta_file.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.05 \
--loss-jt 0.2 \
--loss-cjt 0.8 \
--loss-plv-vel 0.025 \
--loss-jt-vel 0.1 \
--loss-cjt-vel 0.4 \
--vel-loss-mode v1 \
--recons-loss l1_smooth \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_1 \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_1.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.005 \
--loss-jt 0.2 \
--loss-cjt 0.08 \
--loss-plv-vel 0.025 \
--loss-jt-vel 0.1 \
--loss-cjt-vel 0.4 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_2 \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_2.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.005 \
--loss-jt 0.2 \
--loss-cjt 0.08 \
--loss-plv-vel 0.0 \
--loss-jt-vel 0.0 \
--loss-cjt-vel 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_3 \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_3.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.005 \
--loss-jt 0.2 \
--loss-cjt 0.08 \
--loss-plv-vel 0.25 \
--loss-jt-vel 0.1 \
--loss-cjt-vel 0.04 \
--vel-loss-mode v1 \
--recons-loss l1_smooth \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_4 \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_4.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.0 \
--loss-jt 0.0 \
--loss-cjt 0.0 \
--loss-plv-vel 0.0 \
--loss-jt-vel 0.0 \
--loss-cjt-vel 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--use-dynamic-weight \
--norm-dynamic-weight \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_with_normalize_weights \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_with_norm.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.0 \
--loss-jt 0.0 \
--loss-cjt 0.0 \
--loss-plv-vel 0.0 \
--loss-jt-vel 0.0 \
--loss-cjt-vel 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--use-dynamic-weight \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_without_normalize_weights \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_without_norm.log &

# vel loss를 사용하지 않고 확인할 필요도 있음
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.0 \
--loss-jt 0.0 \
--loss-cjt 0.0 \
--loss-plv-vel 0.0 \
--loss-jt-vel 0.0 \
--loss-cjt-vel 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--use-dynamic-weight \
--disable-vel-loss \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_without_normalize_weights_without_vel_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_without_norm_without_vel_loss.log &

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-recons 0.0 \
--loss-vel 0.0 \
--loss-ortho 0.0 \
--loss-gw 0.0 \
--loss-plv 0.0 \
--loss-jt 0.0 \
--loss-cjt 0.0 \
--loss-plv-vel 0.0 \
--loss-jt-vel 0.0 \
--loss-cjt-vel 0.0 \
--vel-loss-mode v3 \
--recons-loss l1_smooth \
--use-dynamic-weight \
--norm-dynamic-weight \
--disable-vel-loss \
--exp-name 2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_with_normalize_weights_without_vel_loss \
--use-keywords \
--output-emb-width 392 
> log_2025_06_10_Dec_ver_joint_group_loss_param_ver_dynamic_with_norm_without_vel_loss.log &


# V2
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--loss-ortho 0.5 \
--recons-loss l1_smooth \
--exp-name 2024_05_19_Dec_with_cla_learnable_pos_emb_both_eval_attn_mask_after_fix_attn_layer_2 \
--use-keywords \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla.yaml
 > log_2024_05_19_train_dec_with_cla_attn_layer_2_plus_orthogonal_loss.log &

# V3 - MLP
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name 2024_04_28_Dec_with_cla_learnable_pos_emb_MLP_aggregator \
--use-keywords \
--use-full-sequence \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla_mlp.yaml
 > train_dec_with_cla_mlp_v1.log &

# Testing
nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir running_test \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name 2024_04_27_Dec_just_test \
--use-keywords \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla.yaml
 > train_dec_with_cla_test.log &



nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name 2024_04_21_Dec_with_cla_pos_emb_sincos_both_eval_attn_mask_after_fix_attn_layer_2_to_1 \
--use-keywords \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla.yaml
 > train_dec_with_cla_v12.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name 2024_04_23_Dec_with_cla_pos_emb_sincos_both_eval_attn_mask_after_fix_attn_reversed_attn_order \
--use-keywords \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla.yaml
 > train_dec_with_cla_v14.log &



nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir my_exp_result \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name 2024_04_21_Dec_vanilla \
--use-keywords \
--output-emb-width 392
 > train_dec_vanilla.log &

## motion vis anlalyze motion reconstruction all

nohup python exp_vis_anl_motion_reconstruction_all.py \
--output-loss-list \
--save-plot \
--save-folder-path /data/CoMo/TEMP_2025_05_24_data_test \
--val-shuffle \
--split test \
--vel-loss-mode v2 > log_2025_05_24_exp_vis_anl_motion_reconstruction_all.log & 

## exp_vis_k_hot_code_change_with_loss_by_frame

nohup python exp_vis_k_hot_code_change_with_loss_by_frame.py \
--save-folder-path /data/CoMo/EXP_analysis_result/2025-05-24/analysis_test_samples > log_2025_06_03_exp_vis_k_hot_code_change_with_loss_by_frame.log & 

## exp_anl_test_model
nohup python exp_anl_test_model.py > lpg_2025_05_28_exp_anl_test_model.log &


# shffule 수행

##  Motion Generator
nohup python train_t2m.py --exp-name Trans --batch-size 64 --num-layers 9 --nb-code 392 --n-head-gpt 16 --block-size 62 --ff-rate 4 --out-dir output --total-iter 300000 --lr-scheduler 150000 --lr 0.0001 --dataname t2m --down-t 2 --depth 3 --eval-iter 10000 --pkeep 0.5 --dilation-growth-rate 3 --output-emb-width 392 --resume-pth ./pretrained/t2m/Dec/model.pth --use-keywords > train_generator.log &


## Motion Decoder(Motion-X)

nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir output_mx_vq \
--dataname mx \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name Dec \
--output-emb-width 392 \
--use-word-only > train_dec_with_data_mx.log &


nohup python train_dec.py \
--batch-size 256 \
--lr 1e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir output_mx_vq \
--dataname mx \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name Dec_with_subset_h3d_fixed \
--output-emb-width 392 \
--use-word-only > train_dec_with_data_mx_v2.log &

#--------- END Training ---------#

#--------- Evaluation ---------#

## Motion Decoder
nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec/exp_vanilla \
--resume-pth ./pretrained/t2m/Dec/model.pth \
--use-keywords \
--output-emb-width 392
> log_eval_dec_2025_06_04_Dec_vanilla.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec_with_CLA \
--resume-pth ./my_exp_result/2024_04_21_Dec_with_cla_learnable_pos_emb_both_eval_attn_mask_after_fix_attn_layer_2_to_1/2025-04-22_13-46-21/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla.yaml 
> eval_dec_3.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--use-full-sequence \
--exp-name TEST_Dec_with_MLP_CLA \
--resume-pth ./my_exp_result/2024_04_28_Dec_with_cla_learnable_pos_emb_MLP_aggregator/2025-04-28_02-12-50/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 \
--cfg-cla-path ./configs/cfg_cla_mlp.yaml 
> eval_dec_mlp.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec_MX_after_fixing_dataset \
--resume-pth ./output_mx_vq/Dec_with_subset_h3d_fixed/2025-04-28_09-20-15/net_best_fid.pth \
--use-keywords \
--output-emb-width 392\ 
> eval_dec_4.log &




nohup python eval_dec_val.py \
--batch-size 32 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name VAL_Dec_MX \
--resume-pth ./output_mx_vq/Dec_with_subset_h3d_fixed/2025-04-28_09-20-15/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 
> eval_dec_mx.log &


nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec/2025_06_04_Dec_ver_joint_group_loss_param_ver_1 \
--resume-pth /data/CoMo/my_exp_result/2025_06_04_Dec_ver_joint_group_loss_param_ver_1/2025-06-04_04-34-51/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 
> log_eval_dec_2025_06_04_Dec_ver_joint_group_loss_param_ver_1.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec/2025_06_04_Dec_ver_joint_group_loss_param_ver_2 \
--resume-pth /data/CoMo/my_exp_result/2025_06_04_Dec_ver_joint_group_loss_param_ver_2/2025-06-04_04-35-29/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 
> log_eval_dec_2025_06_04_Dec_ver_joint_group_loss_param_ver_2.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec/2025_06_04_Dec_ver_joint_group_loss_param_ver_3 \
--resume-pth /data/CoMo/my_exp_result/2025_06_04_Dec_ver_joint_group_loss_param_ver_3/2025-06-04_11-12-10/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 
> log_eval_dec_2025_06_04_Dec_ver_joint_group_loss_param_ver_3.log &

nohup python eval_dec.py \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--nb-code 392 \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--out-dir eval_output \
--dataname t2m \
--vq-act relu \
--loss-vel 0.5 \
--recons-loss l1_smooth \
--exp-name TEST_Dec/2025_06_04_Dec_ver_joint_group_loss_param_ver_4 \
--resume-pth /data/CoMo/my_exp_result/2025_06_04_Dec_ver_joint_group_loss_param_ver_4/2025-06-04_11-16-40/net_best_fid.pth \
--use-keywords \
--output-emb-width 392 
> log_eval_dec_2025_06_04_Dec_ver_joint_group_loss_param_ver_4.log &






## Motion Generator
nohup python eval_t2m.py --exp-name TEST_Base_Trans --batch-size 32 --num-layers 9 --embed-dim-gpt 1024 --nb-code 392 --n-head-gpt 16 --block-size 62 --ff-rate 4 --drop-out-rate 0.1 --resume-pth ./pretrained/t2m/Dec/model.pth --vq-name VQVAE_baseline --out-dir eval_output --total-iter 300000 --lr-scheduler 150000 --lr 0.0001 --dataname t2m --down-t 2 --depth 3 --eval-iter 10000 --pkeep 0.5 --dilation-growth-rate 3 --vq-act relu --output-emb-width 392 --resume-trans ./pretrained/t2m/Trans/model.pth --use-keywords > test_t2m_ver_2.log &


#--------- END Evaluation ---------#

python visualize_token_embedding_CoMo.py --exp-name TEST_Trans --batch-size 32 --num-layers 9 --embed-dim-gpt 1024 --nb-code 392 --n-head-gpt 16 --block-size 62 --ff-rate 4 --drop-out-rate 0.1 --resume-pth ./pretrained/t2m/Dec/model.pth --vq-name VQVAE --out-dir output --total-iter 300000 --lr-scheduler 150000 --lr 0.0001 --dataname t2m --down-t 2 --depth 3 --eval-iter 10000 --pkeep 0.5 --dilation-growth-rate 3 --vq-act relu --output-emb-width 392 --resume-trans ./pretrained/t2m/Trans/model.pth > test.log &

## (Dev) Diffusion
python train_t2m_diffusion.py --cfg /data/CoMo/VQ_diffusion/configs/example.yaml

python train_t2m_diffusion.py --name test --config_file configs/cfg_diffusion_uc.yaml --num_node 1 --tensorboard

nohup python train_t2m_diffusion.py --name test_v3 --config_file configs/cfg_diffusion_uc_light.yaml --num_node 1 --tensorboard > train_diffusion_test_v3.log &



nohup python train_t2m_diffusion.py --name test_v8 --config_file configs/cfg_diffusion_uc_light.yaml --num_node 1 --tensorboard > train_diffusion_test_v8.log &
nohup python train_t2m_diffusion.py --name test_v8 --config_file configs/cfg_diffusion_uc_light.yaml --num_node 1 --tensorboard > train_diffusion_test_v8.log &

nohup python train_t2m_diffusion.py --name test_v13 --config_file configs/cfg_diffusion_SS.yaml --num_node 1 --tensorboard > train_diffusion_test_v13.log &

python train_t2m_diffusion.py --name test_v9 --config_file configs/cfg_diffusion_uc_light.yaml --num_node 1 --tensorboard



nohup python train_t2m_diffusion.py --name test_v20_loss_norm_scaling_fixed --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_v20.log & 


nohup python train_t2m_diffusion.py --name test_v20_loss_norm_scaling_fixed --config_file configs/cfg_diffusion_como_ver.yaml --resume_name test_v20_loss_norm_scaling_fixed --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_v20.log & 
nohup python train_t2m_diffusion.py --name test_v20_2_validation_debug --config_file configs/cfg_diffusion_como_ver.yaml --load_path /data/CoMo/OUTPUT/test_v20_loss_norm_scaling_fixed/checkpoint/last.pth --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_v20_2.log & 


nohup python train_t2m_diffusion.py --name test_v21_validation_loss_added --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_v21.log & 
nohup python train_t2m_diffusion.py --name test_v22_learning_rate_adjust --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_v22.log & 

nohup python train_t2m_diffusion.py --name test_debug_v1_evaluation_wrapper --config_file configs/cfg_diffusion_como_ver_debug.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_debug_v1_eval_wrapper.log & 
nohup python train_t2m_diffusion.py --name test_debug_v2_evaluation_wrapper --config_file configs/cfg_diffusion_como_ver_debug.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_debug_v2_eval_wrapper.log & 
nohup python train_t2m_diffusion.py --name test_debug_v3_evaluation_wrapper --config_file configs/cfg_diffusion_como_ver_debug.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_debug_v3_eval_wrapper.log & 
nohup python train_t2m_diffusion.py --name test_debug_v4_evaluation_wrapper --config_file configs/cfg_diffusion_como_ver_debug.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_debug_v4_eval_wrapper.log & 
nohup python train_t2m_diffusion.py --name test_debug_v5_evaluation_wrapper --config_file configs/cfg_diffusion_como_ver_debug.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_test_debug_v5_eval_wrapper.log & 


nohup python train_t2m_diffusion.py --name train_try_v5 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard > train_diffusion_try_v5.log & 

nohup python train_t2m_diffusion.py --name train_try_v6 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --load_path /data/CoMo/OUTPUT/train_try_v6/checkpoint/last.pth > train_diffusion_try_v6.log & 

nohup python train_t2m_diffusion.py --name train_try_v6 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --load_path /data/CoMo/OUTPUT/train_try_v6/checkpoint/last.pth > train_diffusion_try_v6.log & 

# amp version
nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_32_lr_e_m4 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --amp > train_diffusion_try_using_flash_attn_bs_32_lr_e_m4.log & 

# resume
nohup python train_t2m_diffusion.py --name train_try_v6_2_flash_attn_batch_size_up --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --amp --load_path /data/CoMo/OUTPUT/train_try_v6_2_flash_attn_batch_size_up/checkpoint/last.pth > train_diffusion_try_v6_using_flash_attn_resume.log & 


# 
nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_32_lr_e_m4_layer_9_to_19 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --amp > train_diffusion_try_using_flash_attn_bs_32_lr_e_m4_n_layer_9_to_19.log & 

# 
nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_32_lr_e_m4_hidden_dimension_512_to_1024 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --amp > train_diffusion_try_using_flash_attn_bs_32_lr_e_m4_n_layer_9_to_19_with_hidden_dimension_512_to_1024.log & 

nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_32_lr_e_m4_hidden_dimension_512_to_1024_with_new_embedding_version_v1 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --amp > train_diffusion_try_using_flash_attn_bs_32_lr_e_m4_hidden_512_to_1024_with_new_embdding_version_v1.log & 

# resume 
nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_32_lr_e_m4_hidden_dimension_512_to_1024 --config_file configs/cfg_diffusion_como_ver.yaml --num_node 1 --seed 1234 --tensorboard --amp --load_path /data/CoMo/OUTPUT/train_try_flash_attn_bs_32_lr_e_m4_hidden_dimension_512_to_1024/checkpoint/last.pth > train_diffusion_try_using_flash_attn_bs_32_lr_e_m4_n_layer_9_to_19_with_hidden_dimension_512_to_1024.log & 

# aux loss scale up
nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_32_lr_e_m4_hidden_dimension_512_to_1024_with_aux_loss_scale_up --config_file configs/cfg_diffusion_como_ver_with_aux_loss_up.yaml --num_node 1 --seed 1234 --load_path /data/CoMo/OUTPUT/train_try_flash_attn_bs_32_lr_e_m4_hidden_dimension_512_to_1024_with_aux_loss_scale_up/checkpoint/last.pth --tensorboard --amp > train_diffusion_try_using_flash_attn_bs_32_lr_e_m4_n_layer_9_to_19_with_hidden_dimension_512_to_1024_with_aux_loss_up_resume_2.log & 

# aux loss scale + batch size down + layer up
nohup python train_t2m_diffusion.py --name train_try_flash_attn_bs_16_lr_e_m4_hidden_dimension_512_to_1024_with_aux_loss_scale_up_layer_up_to_16 --config_file configs/cfg_diffusion_como_ver_with_aux_loss_up.yaml --num_node 1 --seed 1234 --tensorboard --amp > train_try_flash_attn_bs_16_lr_e_m4_hidden_dimension_512_to_1024_with_aux_loss_scale_up_layer_up_to_16.log & 


nohup python train_t2m_diffusion.py --name test --config_file configs/cfg_diffusion_como_ver_with_aux_loss_up.yaml --num_node 1 --seed 1234 --tensorboard --amp > test.log & 

# visualize CoMo attention matrix
python visualize_attn_matrix_CoMo.py --exp-name TEST_Trans --batch-size 32 --num-layers 9 --embed-dim-gpt 1024 --nb-code 392 --n-head-gpt 16 --block-size 62 --ff-rate 4 --drop-out-rate 0.1 --resume-pth ./pretrained/t2m/Dec/model.pth --vq-name VQVAE --out-dir output --total-iter 300000 --lr-scheduler 150000 --lr 0.0001 --dataname t2m --down-t 2 --depth 3 --eval-iter 10000 --pkeep 0.5 --dilation-growth-rate 3 --vq-act relu --output-emb-width 392 --resume-trans ./pretrained/t2m/Trans/model.pth > test.log &