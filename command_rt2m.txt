nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v7_retry \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v7_retry.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v8 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v8.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v7 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v7.log &

--> 웜업 추가, 난이도 높여보기, share weight

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v6 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.9 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v6.log &

--> 웜업 추가

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v5 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_w_share_weight_v5.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight_v4 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight_v4.log &

-> 난이도 조절, loss에서 bs 값을 나누는 과정을 제거


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight_v3 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight_v3.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight_v2 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight_v2.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.5 \
--drop-out-rate 0.2 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight.log &



nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.5 \
--drop-out-rate 0.2 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_25 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_25 \
--t2m-checkpoint-folder /data/CoMo/temp_model/exp_2025_09_21_t2m_transformer_vanilla_with_rptc_decoder_2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2_resume/2025-09-27_08-14-05 \
--dec-checkpoint-folder /data/CoMo/my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6_drop_p_ver_2/2025-09-19_04-41-52 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_vanilla_ver_no_pos_emb_on_cond_ver_pad_mask_updated_ver_follow_baseline_wo_share_weight.log &


# --share-weight \