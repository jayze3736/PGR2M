{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86048a3c",
   "metadata": {},
   "source": [
    "# Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f89a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import argparse\n",
    "from os.path import join as pjoin\n",
    "from utils.codebook import *\n",
    "import yaml\n",
    "import visualization.plot_3d_global as plot_3d\n",
    "from utils.motion_process import recover_from_ric \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860d9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfg_ckpt_path(folder_path):\n",
    "    if folder_path is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        ckpt_path = pjoin(folder_path, 'net_best_fid.pth')\n",
    "        config_path = pjoin(folder_path, 'arguments.yaml')\n",
    "    \n",
    "    return config_path, ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b973b60",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1487bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './pretrained/exp_refine_transformer/arguments.yaml'\n",
    "rt2m_checkpoint_folder_path = config_path.replace('arguments.yaml', '')\n",
    "use_gpu = True\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    arg_dict = yaml.safe_load(f)\n",
    "\n",
    "args = argparse.Namespace(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d91114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cpu'), jit=False)  # Must set jit=False for training\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41978c1",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dbaaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading decoder checkpoint from ./pretrained/exp_pg_tokenizer/net_best_fid.pth\n"
     ]
    }
   ],
   "source": [
    "from models.pg_tokenizer import PoseGuidedTokenizer\n",
    "dec_config, dec_checkpoint_path = get_cfg_ckpt_path(args.dec_checkpoint_folder)\n",
    "\n",
    "if dec_config is None or dec_checkpoint_path is None:\n",
    "    raise ValueError(\"Decoder config or checkpoint path is None. Please provide a valid folder path.\")\n",
    "\n",
    "with open(dec_config, 'r') as f:\n",
    "    arg_dict = yaml.safe_load(f)\n",
    "\n",
    "dec_args = argparse.Namespace(**arg_dict)\n",
    "net = PoseGuidedTokenizer(\n",
    "                    dec_args, \n",
    "                    dec_args.nb_code,                      # nb_code\n",
    "                    dec_args.code_dim,                    # code_dim\n",
    "                    dec_args.output_emb_width,            # output_emb_width\n",
    "                    dec_args.down_t,                      # down_t\n",
    "                    dec_args.stride_t,                    # stride_t\n",
    "                    dec_args.width,                       # width\n",
    "                    dec_args.depth,                       # depth\n",
    "                    dec_args.dilation_growth_rate,        # dilation_growth_rate\n",
    "                    dec_args.vq_act,                      # activation\n",
    "                    dec_args.vq_norm,                     # norm\n",
    "                    num_quantizers=dec_args.rvq_num_quantizers,\n",
    "                    shared_codebook=dec_args.rvq_shared_codebook,\n",
    "                    quantize_dropout_prob=dec_args.rvq_quantize_dropout_prob,\n",
    "                    quantize_dropout_cutoff_index=dec_args.rvq_quantize_dropout_cutoff_index,\n",
    "                    rvq_nb_code=dec_args.rvq_nb_code,\n",
    "                    mu=dec_args.rvq_mu,\n",
    "                    residual_ratio=dec_args.rvq_residual_ratio,\n",
    "                    vq_loss_beta=dec_args.rvq_vq_loss_beta,\n",
    "                    quantizer_type=dec_args.rvq_quantizer_type,\n",
    "                    params_soft_ent_loss=dec_args.params_soft_ent_loss,\n",
    "                    use_ema=(not dec_args.unuse_ema),\n",
    "                    init_method=dec_args.rvq_init_method\n",
    "                    )\n",
    "    \n",
    "print ('loading decoder checkpoint from {}'.format(dec_checkpoint_path))\n",
    "ckpt = torch.load(dec_checkpoint_path, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "\n",
    "net.eval()\n",
    "if use_gpu:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb63ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Token Emb: Linear Token Embedding Selected ##\n",
      "loading transformer checkpoint from ./pretrained/exp_base_transformer/net_best_fid.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseTrans(\n",
       "  (trans_base): CrossCondTransBase(\n",
       "    (tok_emb): Linear(in_features=394, out_features=1024, bias=True)\n",
       "    (cond_emb): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (pos_embedding): Embedding(62, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embed): PositionEmbedding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (trans_head): CrossCondTransHead(\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1024, out_features=394, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.t2m_trans as t2m\n",
    "t2m_config, t2m_checkpoint_path = get_cfg_ckpt_path(args.t2m_checkpoint_folder)\n",
    "\n",
    "if t2m_config is None or t2m_checkpoint_path is None:\n",
    "    raise ValueError(\"Decoder config or checkpoint path is None. Please provide a valid folder path.\")\n",
    "\n",
    "with open(t2m_config, 'r') as f:\n",
    "    arg_dict = yaml.safe_load(f)\n",
    "\n",
    "t2m_args = argparse.Namespace(**arg_dict)\n",
    "trans_net = t2m.BaseTrans(num_vq=t2m_args.nb_code, \n",
    "                        embed_dim=t2m_args.embed_dim_gpt, \n",
    "                        clip_dim=t2m_args.clip_dim, \n",
    "                        block_size=t2m_args.block_size, \n",
    "                        num_layers=t2m_args.num_layers, \n",
    "                        n_head=t2m_args.n_head_gpt, \n",
    "                        drop_out_rate=t2m_args.drop_out_rate, \n",
    "                        fc_rate=t2m_args.ff_rate)\n",
    "\n",
    "print ('loading transformer checkpoint from {}'.format(t2m_checkpoint_path))\n",
    "trans_ckpt = torch.load(t2m_checkpoint_path, map_location='cpu')\n",
    "trans_net.load_state_dict(trans_ckpt['trans'], strict=True)\n",
    "\n",
    "if use_gpu:\n",
    "    trans_net.cuda()\n",
    "trans_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e688576",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_reference = 'net_best_fid'\n",
    "r_trans_config, r_trans_checkpoint_path = get_cfg_ckpt_path(rt2m_checkpoint_folder_path)\n",
    "\n",
    "if r_trans_config is None or r_trans_checkpoint_path is None:\n",
    "    raise ValueError(\"Residual Transformer config or checkpoint path is None. Please provide a valid folder path.\")\n",
    "\n",
    "with open(r_trans_config, 'r') as f:\n",
    "    arg_dict = yaml.safe_load(f)\n",
    "\n",
    "r_trans_args = argparse.Namespace(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e9b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading transformer checkpoint from ./pretrained/exp_refine_transformer/net_best_fid.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.rt2m_trans import RefineTrans\n",
    "\n",
    "res_trans_net = RefineTrans(num_vq=r_trans_args.nb_code, \n",
    "                                    num_rvq=dec_args.rvq_nb_code,\n",
    "                                    embed_dim=r_trans_args.embed_dim_gpt, \n",
    "                                    clip_dim=r_trans_args.clip_dim, \n",
    "                                    block_size=r_trans_args.block_size, \n",
    "                                    num_layers=r_trans_args.num_layers, \n",
    "                                    n_head=r_trans_args.n_head_gpt, \n",
    "                                    drop_out_rate=r_trans_args.drop_out_rate, \n",
    "                                    fc_rate=r_trans_args.ff_rate,\n",
    "                                    num_key=11,\n",
    "                                    mode=None,\n",
    "                                    num_quantizer=dec_args.rvq_num_quantizers,\n",
    "                                    share_weight=r_trans_args.share_weight)\n",
    "\n",
    "\n",
    "print ('loading transformer checkpoint from {}'.format(r_trans_checkpoint_path))\n",
    "r_trans_ckpt = torch.load(r_trans_checkpoint_path, map_location='cpu')\n",
    "res_trans_net.load_state_dict(r_trans_ckpt['r_trans'], strict=True)\n",
    "\n",
    "# eval mode로 고정\n",
    "if use_gpu:\n",
    "    res_trans_net.cuda()\n",
    "res_trans_net.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9ce3f",
   "metadata": {},
   "source": [
    "# Test Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555a7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Note!: Validation Shuffle :True\n",
      "# Codes Folder name: codes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:03<00:00, 1305.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------- RESULT ---------#\n",
      "split:test\n",
      "186 files filtered\n",
      "0 files occured an error\n",
      "#--------- END ---------#\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "from dataset import dataset_TM_eval # \n",
    "\n",
    "is_test = True\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, is_test, 32, w_vectorizer, codebook_size=392,\n",
    "                                        use_keywords=args.use_keywords,\n",
    "                                        use_word_only=args.use_word_only,\n",
    "                                        codes_folder_name=args.codes_folder_name)\n",
    "\n",
    "val_loader_iter = dataset_TM_eval.cycle(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efab48",
   "metadata": {},
   "source": [
    "# Search Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f834dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test samples: 4384\n"
     ]
    }
   ],
   "source": [
    "split = 'test'\n",
    "with open(f'./dataset/HumanML3D/{split}.txt', 'r') as f:\n",
    "    split_file_names = f.read().splitlines()\n",
    "print(f\"number of {split} samples: {len(split_file_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e7fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: 011965 text: a person abruptly stumbles forward and regains his balance as if he had been pushed from behind.\n",
      "name: M013056 text: a person angrily and quickly walks then abruptly stops with force.\n",
      "name: 013249 text: a person walks diagonally forward, stops abruptly, grabs his head with his left hand, walks backward briefly, and then turn to the left and walks straight.\n",
      "name: M012488 text: a person abruptly staggers backwards.\n",
      "name: M011965 text: a person abruptly stumbles forward and regains his balance as if he had been pushed from behind.\n",
      "name: M013249 text: a person walks diagonally forward, stops abruptly, grabs his head with his right hand, walks backward briefly, and then turn to the right and walks straight.\n",
      "name: 000067 text: a person casually walks abruptly steps left and then recovers to a straight walk.\n",
      "name: M004472 text: a person who is standing is getting pushed by something and moving abruptly.\n",
      "name: M000067 text: a person casually walks abruptly steps right and then recovers to a straight walk.\n",
      "name: 012488 text: a person abruptly staggers backwards.\n",
      "name: M011334 text: person walks forward speedily then abruptly stops\n",
      "name: M000253 text: a person walks forward, turns around abruptly, then walks back to the original position.\n",
      "name: M011442 text: a person walks slowly in a circle but then jumps back abruptly and continues to hop around.\n",
      "name: 011652 text: a person jumps in the air, then abruptly stumbles to his left as if he had been pushed, and finally he regains his balance.\n",
      "name: 004719 text: a person runs forward then abruptly turns to the left and continues running\n",
      "name: 004472 text: a person who is standing is getting pushed by something and moving abruptly.\n",
      "name: 000253 text: a person walks forward, turns around abruptly, then walks back to the original position.\n",
      "name: M011652 text: a person jumps in the air, then abruptly stumbles to his right as if he had been pushed, and finally he regains his balance.\n",
      "name: 011442 text: a person walks slowly in a circle but then jumps back abruptly and continues to hop around.\n",
      "name: M004719 text: a person runs forward then abruptly turns to the right and continues running\n",
      "name: 013056 text: a person angrily and quickly walks then abruptly stops with force.\n",
      "name: 011334 text: person walks forward speedily then abruptly stops\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "query = ['abruptly']\n",
    "text_dir = './dataset/HumanML3D/texts'\n",
    "\n",
    "file_names = []\n",
    "texts = []\n",
    "for file in os.listdir(text_dir):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(f\"{text_dir}/{file}\", 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                matched = any(phrase in line for phrase in query)\n",
    "                if matched:\n",
    "                    file_name = file.replace('.txt', '')\n",
    "                    if file_name in split_file_names:\n",
    "                        texts.append(line.split('#')[0])\n",
    "                        file_names.append(file_name)\n",
    "                        print(\"name:\", file_name, \"text:\", line.split('#')[0])\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "searched_file_ids = [file_name for file_name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118c9323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person walks forward speedily then abruptly stops']\n"
     ]
    }
   ],
   "source": [
    "target_file_list = ['011334'] # e.g., ['M004719']\n",
    "text_dir = './dataset/HumanML3D/texts'\n",
    "texts = []\n",
    "pose_list = []\n",
    "\n",
    "for id in target_file_list:\n",
    "    with open(f\"{text_dir}/{id}.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sentence = line.strip().split('#')[0]\n",
    "            if query[0] in sentence:\n",
    "                texts.append(sentence)\n",
    "                break\n",
    "\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba637a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name: 011334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "m_length_list = []\n",
    "search_mode = True\n",
    "unit_length = 4\n",
    "keyword_embeddings = None\n",
    "\n",
    "keyword_emb_dir = './dataset/HumanML3D/keyword_embeddings'\n",
    "pose_dir = './dataset/HumanML3D/new_joint_vecs'\n",
    "\n",
    "for file_name in target_file_list:\n",
    "    print(\"file_name:\", file_name)\n",
    "    emb = np.load(os.path.join(keyword_emb_dir, f'{file_name}.npy'))\n",
    "    keyword_indices = np.arange(0,33,3) + np.random.randint(0,3,size = (11,))    \n",
    "    keyword_data = torch.from_numpy(emb[keyword_indices]).unsqueeze(0) # 변환 작업\n",
    "\n",
    "    pose = torch.from_numpy(np.load(os.path.join(pose_dir, f'{file_name}.npy'))).unsqueeze(0)\n",
    "    pose_list.append(val_loader.dataset.forward_transform(pose))\n",
    "    m_length_list.append((pose.shape[1] // unit_length) * unit_length)\n",
    "    \n",
    "    if keyword_embeddings is None:\n",
    "        keyword_embeddings = keyword_data\n",
    "    else:\n",
    "        keyword_embeddings = torch.cat([keyword_embeddings, keyword_data], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60104878",
   "metadata": {},
   "source": [
    "# Custom Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e61e1023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time :  2025-12-15 15-20-26\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "print(\"Current date and time : \", date_time)\n",
    "\n",
    "text = clip.tokenize(texts, truncate=True) # .cuda('cuda:1')\n",
    "feat_clip_text = clip_model.encode_text(text).float().unsqueeze(1) \n",
    "feat_clip_text = torch.cat((feat_clip_text, keyword_embeddings.float()), dim=1)  # bs x 11+1 x 512\n",
    "feat_clip_text = feat_clip_text.cuda()\n",
    "\n",
    "split = 'test'\n",
    "\n",
    "save_dir = f'./inference/{date_time}/{split}/'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53743826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_clip_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5460f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "pred_p_code_list = []\n",
    "pred_r_code_list = []\n",
    "name_list = target_file_list\n",
    "clip_text_list = []\n",
    "clip_text = texts\n",
    "\n",
    "for k in range(len(target_file_list)):\n",
    "    print(f\"iter:{k}\")\n",
    "    pred_p_codes, pred_r_codes = res_trans_net.sample(feat_clip_text[k:k+1], trans_net) # 1 x t x code_num -> k-hot vector (bs, seq_len, 394)\n",
    "    \n",
    "    pred_p_code_list.append(pred_p_codes)\n",
    "    pred_r_code_list.append(pred_r_codes)\n",
    "    name_list.append(name[k])\n",
    "    clip_text_list.append(clip_text[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a14b671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_joints = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c00483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_custom_inference(pose, val_loader, sentence, save_dir, sample_name, save_name):\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pose.detach().cpu().numpy())\n",
    "    pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float(), num_joints)\n",
    "    pred_xyz = pred_xyz.numpy()\n",
    "    print(pred_xyz.shape)\n",
    "    os.makedirs(f\"{save_dir}/{sample_name}\", exist_ok=True)\n",
    "    np.save(f\"{save_dir}/{sample_name}/{save_name}_coord.npy\", pred_xyz)\n",
    "    np.save(f\"{save_dir}/{sample_name}/{save_name}_pose.npy\", pose.cpu().detach().numpy())\n",
    "    save_path = f\"{save_dir}/{sample_name}/{save_name}.gif\"\n",
    "    plot_3d.draw_to_batch(pred_xyz, [sentence], [save_path], footer_text=f'{save_name}', footer_fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b7e12f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 011334\n",
      "(1, 68, 22, 3)\n",
      "(1, 120, 22, 3)\n",
      "(1, 120, 22, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:39<00:00, 39.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(range(len(pred_p_code_list))):\n",
    "    sentence = clip_text_list[k]\n",
    "    pred_pose_codes = pred_p_code_list[k][..., :-2]\n",
    "    pred_residual_codes = pred_r_code_list[k]\n",
    "    gt_pose = pose_list[k]\n",
    "    name = name_list[k]\n",
    "\n",
    "    print(f\"sample: {name}\")\n",
    "\n",
    "    ##################\n",
    "    gt_pose = gt_pose[:, :m_length_list[k], :]\n",
    "    postprocess_custom_inference(gt_pose, val_loader, sentence, save_dir, name, 'ground_truth')\n",
    "\n",
    "    ##################\n",
    "    pred_pose = net.inference(code_indices=pred_pose_codes.float(), residual_codes=pred_residual_codes.float(), drop_out_residual_quantization=True)\n",
    "    postprocess_custom_inference(pred_pose, val_loader, sentence, save_dir, name, 'wo_residual')\n",
    "\n",
    "    #################\n",
    "    pred_pose = net.inference(pred_residual_codes.float(), pred_pose_codes.float())\n",
    "    postprocess_custom_inference(pred_pose, val_loader, sentence, save_dir, name, 'w_residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb6c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_como (3.8.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
