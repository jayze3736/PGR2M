# 현재 진행중
# Notice! 
1. 
2. 
3. 

nohup python train_rt2m.py \
--exp-name exp_2025_12_05_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--t2m-checkpoint-folder ./important_result/Ours/Trans/exp_2025_11_15_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_1 \
--dec-checkpoint-folder ./important_result/Ours/Dec/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6 \
--use-keywords > log_exp_2025_12_05_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb.log &


nohup python train_rt2m.py \
--exp-name exp_2025_11_17_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_11_17_cb_size_512 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_11_17_cb_size_512 \
--t2m-checkpoint-folder ./output/exp_2025_11_15_t2m_transformer_vanilla_original_ver_soft_dec_cb_512_q_layer_6_drop_ver_1/2025-11-15_08-09-03 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6/2025-09-27_12-54-44 \
--use-keywords > log_exp_2025_11_17_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb.log &


nohup python train_rt2m.py \
--exp-name exp_2025_11_16_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb_w_dataset_kit \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 51 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname kit \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_kit_unit_len_4_2025_11_16_cb_size_512 \
--eval-cache-file-name TM_split_val_kit_unit_len_4_2025_11_16_cb_size_512 \
--t2m-checkpoint-folder ./output/exp_2025_11_13_kit_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2/2025-11-14_02-46-57 \
--dec-checkpoint-folder ./my_exp_result/2025_11_13_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_dataname_kit/2025-11-13_11-06-05 \
--start-warm-up > log_exp_2025_11_16_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb_w_dataset_kit.log &



nohup python train_rt2m.py \
--exp-name exp_2025_11_15_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb_w_pretrained_baseline_model \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_11_15_cb_size_512 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_11_15_cb_size_512 \
--t2m-checkpoint-folder ./output/exp_2025_11_13_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_1/2025-11-13_05-29-43 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6/2025-09-27_12-54-37 \
--use-keywords > log_exp_2025_11_15_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb_w_pretrained_baseline_model.log &

nohup python train_rt2m.py \
--exp-name TEST_exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb_w_pretrained_baseline_model \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 63 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_11_14_cb_size_512 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_11_14_cb_size_512 \
--t2m-checkpoint-folder ./important_result/Baseline/Trans \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > TEST_log_exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb_w_pretrained_baseline_model.log &



nohup python train_rt2m.py \
--exp-name exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb_v3 \
--batch-size 64 \
--num-layers 4 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.85 \
--masking-prob 0.1 \
--drop-out-rate 0.1 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--t2m-checkpoint-folder ./output/exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2/2025-10-30_23-41-47 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb_v3.log &

## corrupting pose code + residual code

nohup python train_rt2m.py \
--exp-name exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_256_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb_v2 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_11_02_cb_size_256 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_11_02_cb_size_256 \
--t2m-checkpoint-folder ./output/exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_256_q_layer_6_drop_ver_1/2025-10-31_01-12-05 \
--dec-checkpoint-folder ./my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6/2025-09-19_02-40-58 \
--use-keywords > log_exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_256_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb_v2.log &

nohup python train_rt2m.py \
--exp-name exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 150000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--t2m-checkpoint-folder ./output/exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2/2025-10-30_23-41-47 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_w_share_weight_v9_ver_pre_pose_code_emb.log &

nohup python train_rt2m.py \
--exp-name exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_256_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--t2m-checkpoint-folder ./output/exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_256_q_layer_6_drop_ver_1/2025-10-31_01-12-05 \
--dec-checkpoint-folder ./my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6/2025-09-19_02-40-58 \
--use-keywords > log_exp_2025_11_02_rt2m_transformer_ver_dec_cb_size_256_res_drop_ver_1_ver_w_share_weight_v9_ver_pre_pose_code_emb.log &



nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_ver_real_3 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27_cb_size_256 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27_cb_size_256 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_ver_real_3.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_ver_real_2 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_ver_real_2.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_ver_real \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_ver_real.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_v3 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.1 \
--warm-up-iter 5000 \
--start-warm-up \
--load-pretrained-pose-code-emb \
--share-weight \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_v3.log &

# --freeze-pose-code-emb \ 제거

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_v2 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 1 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.1 \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9_v2.log &

# v9 config + ff rate 4 -> 1, share weight 제거

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_wo_share_weight_v12 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--gamma 0.05 \
--warm-up-iter 5000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_wo_share_weight_v12.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v11 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 51 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--warm-up-iter 2000 \
--gamma 0.05 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--start-warm-up > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v11.log &

## keyword embedding 삭제, v1 config + share weight 삭제

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v10 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 51 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--warm-up-iter 2000 \
--start-warm-up \
--gamma 0.05 \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--share-weight > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v10.log &

## keyword embedding 삭제, v1 config


# v7은 pkeep에만 노이즈가 추가 되어됨

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 1024 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 5000 \
--start-warm-up \
--freeze-pose-code-emb \
--load-pretrained-pose-code-emb \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v9.log &

## --mask-resdual-code \

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v8_no_sample_for_masking_res_code \
--batch-size 64 \
--num-layers 12 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--gamma 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.9 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v8_no_sample_for_masking_res_code.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v7 \
--batch-size 64 \
--num-layers 12 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--gamma 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v7.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v6 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--gamma 0.5 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v6.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v5 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v5.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v4 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 8 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 512 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 80000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v4.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v3 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v3.log &

nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v2 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.9 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.1 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v2.log &


nohup python train_rt2m.py \
--exp-name exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v1 \
--batch-size 64 \
--num-layers 6 \
--nb-code 392 \
--n-head-gpt 6 \
--block-size 62 \
--ff-rate 4 \
--embed-dim-gpt 384 \
--out-dir output_residual_transformer \
--total-iter 300000 \
--lr-scheduler 50000 \
--lr 0.0002 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 1.0 \
--mask-residual-code \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_rt2m/losses_v1.yaml \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--masking-prob 0.0 \
--drop-out-rate 0.2 \
--share-weight \
--warm-up-iter 2000 \
--start-warm-up \
--train-cache-file-name TM_split_train_t2m_unit_len_4_2025_10_27 \
--eval-cache-file-name TM_split_val_t2m_unit_len_4_2025_10_27 \
--t2m-checkpoint-folder ./output/exp_2025_10_25_t2m_transformer_vanilla_update_loss_ver_dec_res_drop_cb_size_512_q_layer_6_res_drop_ver_2/2025-10-25_12-37-34 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_26_rt2m_transformer_ver_dec_cb_size_512_res_drop_ver_2_ver_follow_baseline_w_share_weight_v1.log &
