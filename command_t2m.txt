nohup python train_t2m_original.py \
--exp-name exp_2025_12_04_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_hard_quantizer_drop_ver_2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-42 \
--use-keywords > log_exp_2025_12_04_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_hard_quantizer_drop_ver_2.log &

nohup python train_t2m_original.py \
--exp-name exp_2025_11_21_t2m_transformer_vanilla_original_ver_soft_dec_cb_512_q_layer_1_drop_ver_2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_11_21_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_1_drop_p_ver_2/2025-11-20_20-55-58 \
--use-keywords > log_exp_2025_11_21_t2m_transformer_vanilla_original_ver_soft_dec_cb_512_q_layer_1_drop_ver_2.log &


nohup python train_t2m_original.py \
--exp-name exp_2025_11_15_t2m_transformer_vanilla_original_ver_soft_dec_cb_512_q_layer_6_drop_ver_1 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6/2025-09-27_12-54-44 \
--use-keywords > log_exp_2025_11_15_t2m_transformer_vanilla_original_ver_soft_dec_cb_512_q_layer_6_drop_ver_1.log &


nohup python train_t2m_original.py \
--exp-name exp_2025_11_14_kit_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_1 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname kit \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_11_13_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_1_dataname_kit/2025-11-13_11-06-03 \
--eval-masking > log_exp_2025_11_13_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_1.log &

nohup python train_t2m_original.py \
--exp-name exp_2025_11_13_kit_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname kit \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_11_13_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2_dataname_kit/2025-11-13_11-06-05 \
--eval-masking > log_exp_2025_11_13_kit_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2.log &


nohup python train_t2m_original.py \
--exp-name exp_2025_11_13_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_1 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_cb_size_512_num_quantizer_6/2025-09-27_12-54-37 \
--use-keywords > log_exp_2025_11_13_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_1.log &


nohup python train_t2m_original.py \
--exp-name exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2_try_fix_seed \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2_try_fix_seed.log &



nohup python train_t2m_original.py \
--exp-name exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2_try_fix_seed \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2_try_fix_seed.log &


nohup python train_t2m_original.py \
--exp-name exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_256_q_layer_6_drop_ver_1 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_19_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_256_num_quantizer_6/2025-09-19_02-40-58 \
--use-keywords > log_exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_256_q_layer_6_drop_ver_1.log &

nohup python train_t2m_original.py \
--exp-name exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2 \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 10000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./my_exp_result/2025_09_27_Dec_ver_using_residual_pose_temporal_complementor_ver_res_drop_detach_p_latent_with_soft_quantizer_try_12_unuse_ema_cb_size_512_num_quantizer_6_drop_p_ver_2/2025-09-27_12-49-50 \
--use-keywords > log_exp_2025_10_31_t2m_transformer_vanilla_original_ver_dec_cb_512_q_layer_6_drop_ver_2.log &

nohup python train_t2m_update_loss.py \
--exp-name exp_2025_10_26_t2m_transformer_vanilla_update_loss_focal_bce_loss_no_pos_emb_on_cond \
--batch-size 64 \
--num-layers 9 \
--nb-code 392 \
--n-head-gpt 16 \
--block-size 62 \
--ff-rate 4 \
--out-dir output \
--total-iter 300000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--eval-iter 5000 \
--pkeep 0.5 \
--num_workers 16 \
--dilation-growth-rate 3 \
--output-emb-width 392 \
--val-shuffle \
--loss-cfg-path /data/CoMo/configs/exp_t2m_v2/losses_v2.yaml \
--log-cat-right-num \
--eval-masking \
--min-sampling-prob 0.1 \
--dec-checkpoint-folder ./pretrained/t2m/Dec \
--use-keywords > log_exp_2025_10_26_t2m_transformer_vanilla_update_loss_focal_bce_loss_no_pos_emb_on_cond.log &